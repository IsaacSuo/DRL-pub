{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gBUF2HJd1fVT"
   },
   "outputs": [],
   "source": [
    "Student_1 = '' #@param {type:\"string\"}  Name of student 1\n",
    "Student_2 = '' #@param {type:\"string\"}  Name of student 2\n",
    "Student_3 = 'HE, Yunhan' #@param {type:\"string\"}  Name of student 3\n",
    "Student_4 = 'XIE, Qingyun' #@param {type:\"string\"}  Name of student 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9PhWOSD2hLUM"
   },
   "source": [
    "# Overview\n",
    "\n",
    "This is the skeleton code file for the EEC4400 assignment. Replace the `XX`. in the name of this Jupyter notebook with your group number (this is important for correct marks to be awarded to your group). Fill in the blank cells below with the necessary code (you should work on this Jupyter notebook section by section). At the end, the entire Jupyter notebook should generate all the required results and execute without error.\n",
    "\n",
    "The text above the blank cells provides information on the functionality that needs to be implemented. You need to write Python code at places indicated by `[WriteCode]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jXckzF3Fhd4p"
   },
   "source": [
    "## Install and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "E669iRUFhn9J"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-02 16:33:53.274862: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-11-02 16:33:53.275081: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-02 16:33:53.298336: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-02 16:33:53.954740: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-02 16:33:53.955024: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1762072434.280804 3030909 cuda_executor.cc:1309] INTERNAL: CUDA Runtime error: Failed call to cudaGetRuntimeVersion: Error loading CUDA libraries. GPU will not be used.: Error loading CUDA libraries. GPU will not be used.\n",
      "W0000 00:00:1762072434.287587 3030909 gpu_device.cc:2342] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "import gymnasium as gym\n",
    "device = \"cuda\" if tf.config.list_physical_devices('GPU') else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9d3w4FRMS36_"
   },
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8RD1Ggwcqg1"
   },
   "source": [
    "## Environment Exploration: Cart-Pole\n",
    "\n",
    "The **Cart-Pole environment** is a classic reinforcement learning problem where a pole is attached by an un-actuated joint to a cart. The cart moves along a frictionless track, and the goal is to balance the pole upright by applying forces to move the cart left or right.\n",
    "\n",
    "<img src=\"https://gymnasium.farama.org/_images/cart_pole.gif\" width=\"400\">\n",
    "\n",
    "Below is a breakdown of the base Cart-Pole environment:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geWhWkTJRu5w"
   },
   "source": [
    "#### - **Observation Space (*s*)**\n",
    "The observation is represented as a `ndarray` of shape `(4,)`, corresponding to:\n",
    "- **Cart Position**: Horizontal location of the cart.\n",
    "- **Cart Velocity**: Speed of the cart along the track.\n",
    "- **Pole Angle**: Angular position of the pole relative to vertical.\n",
    "- **Pole Angular Velocity**: Speed at which the pole angle changes.\n",
    "\n",
    "The observations are initialized with uniformly random values in the range `(-0.05, 0.05)`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8r3befVJR4SJ"
   },
   "source": [
    "#### - **Action Space (*a*)**\n",
    "The action is a discrete value (`0` or `1`) indicating the direction of the force applied to the cart:\n",
    "- `0`: Push the cart to the left.\n",
    "- `1`: Push the cart to the right.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BfanxRL4R5jO"
   },
   "source": [
    "\n",
    "#### - **Reward Function (*r*)**\n",
    "The agent receives a reward of `+1` for each time step it successfully keeps the pole upright. The maximum achievable reward in a single episode is `500`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "spv7Ul4CR7SN"
   },
   "source": [
    "#### - **Episode Termination (*isDone*)**\n",
    "An episode ends if any of the following conditions occur:\n",
    "1. **Pole Angle** exceeds `±12°`.\n",
    "2. **Cart Position** exceeds `±2.4` (the cart reaches the edge of the track).\n",
    "3. **Truncation**: The episode reaches the maximum length of `500 steps`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pD50TBeLif6F"
   },
   "source": [
    "## Getting Familiar with Basic Gymnasium Usage\n",
    "\n",
    "Let's observe how to interact with a Gymnasium environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kkeWe780XZVv"
   },
   "source": [
    "The classic “agent-environment loop” pictured below is a simplified representation of reinforcement learning that Gymnasium implements.\n",
    "\n",
    "![agt-env.png](assets/agt-env.png)\n",
    "\n",
    "The following section represents a simple episode of this loop, we will:\n",
    "\n",
    "1. Import the base Cart-Pole environment from the gymnasium library and explore its functionality.\n",
    "\n",
    "2. Apply modifications to make the environment more challenging by adjusting the pole's dynamics and introducing stochastic forces.\n",
    "\n",
    "3. Sample random actions to control the cart-pole.\n",
    "\n",
    "These steps will help us understand how the environment behaves under different conditions and prepare us for implementing reinforcement learning agents in later sections.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "x2PMYpIKXtWv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Exploring the Base Environment ---\n",
      "Initial Observation: [ 0.0076304  -0.04306766 -0.03560165 -0.00620851]\n",
      "Action Space: Discrete(2)\n",
      "Observation Space: Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import and Explore the Environment\n",
    "print(\"\\n--- Exploring the Base Environment ---\")\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "observation, info = env.reset()\n",
    "\n",
    "# Print information about the environment\n",
    "print(f\"Initial Observation: {observation}\")\n",
    "print(f\"Action Space: {env.action_space}\")  # Discrete actions: 0 (left), 1 (right)\n",
    "print(f\"Observation Space: {env.observation_space}\")  # State variables: cart position, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9wjDE0kfFJd"
   },
   "source": [
    "# Define Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0UFVPRylCdv"
   },
   "source": [
    "## Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "vsmmRtAVlUz_"
   },
   "outputs": [],
   "source": [
    "# Some plotting functions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_eval_rwd_mean(eval_mean_list):\n",
    "    \"\"\"Plot evaluation reward mean.\"\"\"\n",
    "    # [WriteCode]\n",
    "\n",
    "\n",
    "\n",
    "def plot_eval_rwd_var(eval_var_list):\n",
    "    \"\"\"Plot evaluation reward variance.\"\"\"\n",
    "    # [WriteCode]\n",
    "\n",
    "\n",
    "\n",
    "def plot_smoothed_training_rwd(train_rwd_list, window_size=20):\n",
    "    \"\"\"Plot smoothed training rewards using a moving average.\"\"\"\n",
    "    # [WriteCode]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N7hA67eaYkCW"
   },
   "source": [
    "## Evaluation Function\n",
    "\n",
    "Evaluate the learnt policy by running 5 evaluation episodes and computing the average and variance of rewards collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "kKS1vpSmYjyg"
   },
   "outputs": [],
   "source": [
    "def evaluation(model, max_timesteps=500):\n",
    "    eval_env = gym.make(\"CartPole-v1\")\n",
    "    state_size = eval_env.observation_space.shape[0] # Number of observations (CartPole)\n",
    "    action_size = eval_env.action_space.n            # Number of possible actions\n",
    "    eval_reward = []\n",
    "\n",
    "    for i in range (5):\n",
    "        round_reward = 0\n",
    "        state, _ = eval_env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "\n",
    "        for i in range(max_timesteps):\n",
    "            action = np.argmax(model.predict(state, verbose=0)[0])\n",
    "            next_state, reward, terminated, truncated, _ = eval_env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "\n",
    "            round_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "            if terminated or truncated:\n",
    "                eval_reward.append(round_reward)\n",
    "                break\n",
    "\n",
    "    eval_env.close()\n",
    "\n",
    "    eval_reward_mean = np.sum(eval_reward)/len(eval_reward)\n",
    "    eval_reward_var = np.var(eval_reward)\n",
    "\n",
    "    return eval_reward_mean, eval_reward_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kWS8Kd8eiXau"
   },
   "source": [
    "# Setting up Tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ADO90Q7Hs-lx"
   },
   "outputs": [],
   "source": [
    "def get_run_logdir(k):\n",
    "    root_logdir = os.path.join(os.curdir, \"eec4400_logs\", k)\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B07kopgXtOVU"
   },
   "source": [
    "# Initialize Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "JL9ABa8-vRf3"
   },
   "outputs": [],
   "source": [
    "# Use the following set of NN hyperparameters for ALL FOUR baseline policies\n",
    "lr =  1e-4        #@param {type:\"number\"}               # learning rate\n",
    "epoch =  4     #@param {type:\"number\"}               # epochs\n",
    "episode = 250  #@param {type:\"number\"}               # episodes\n",
    "\n",
    "epsilon = 0.7           #@param {type:\"number\"}     # Starting exploration rate\n",
    "epsilon_min = 0.01    #@param {type:\"number\"}     # Exploration rate min\n",
    "epsilon_decay = 0.1     #@param {type:\"number\"}     # Exploration rate decay\n",
    "\n",
    "gamma = 0.99          #@param {type:\"number\"}     # Agent discount factor\n",
    "\n",
    "# Use the following set of NN hyperparameters for Naive DQN, DQN and DDQN policies\n",
    "ba =  32      #@param {type:\"number\"}               # batch_size\n",
    "\n",
    "# Use the following set of RL hyperparameters for DQN and DDQN policies\n",
    "target_update_freq = 10 # @param {type:\"number\"}    # Target network update frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingConfig(lr=0.0001, epoch=4, episode=250, epsilon=0.2, epsilon_min=1e-05, epsilon_decay=0.5, gamma=0.99, ba=32, target_update_freq=10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from config.train_cfg import TrainingConfig\n",
    "\n",
    "train_cfg = TrainingConfig(\n",
    "    lr=lr,\n",
    "    epoch=epoch,\n",
    "    episode=episode,\n",
    "    epsilon=epsilon,\n",
    "    epsilon_min=epsilon_min,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    gamma=gamma,\n",
    "    ba=ba,\n",
    "    target_update_freq=target_update_freq\n",
    ")\n",
    "train_cfg.load('config/hyperparams.yml')\n",
    "train_cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "npUYex5Em46O"
   },
   "source": [
    "# Q-Network - Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLr4PDQ7nC5T"
   },
   "source": [
    "## Define and Compile the Neural Network\n",
    "\n",
    "A single network $Q_\\theta$ (parameterized by $\\theta$) is used to approximate $Q(s,a)$.\n",
    "\n",
    "The target used by Naive DQN is then:\n",
    "\n",
    "$Y^{NaiveQ}_t = R_{t+1} + \\gamma Q_{\\theta}(S_{t+1}, a)$\n",
    "\n",
    "The training of Q-Network does NOT rely on a Replay Buffer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "WyXho4UKpH3Q"
   },
   "outputs": [],
   "source": [
    "# Q-Network Baseline Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "\n",
    "# [WriteCode] from ... import ...\n",
    "\n",
    "\n",
    "# Define the Q-network\n",
    "model = Sequential()\n",
    "\n",
    "# [WriteCode]\n",
    "# model.add(...\n",
    "\n",
    "# Compile the model\n",
    "# [WriteCode]\n",
    "\n",
    "# Print the model summary\n",
    "# [WriteCode]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PunGDA41nK-t"
   },
   "source": [
    "## Set Up Env and Train the Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "-cDAJtaGnCQL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.0000 seconds per episode\n"
     ]
    }
   ],
   "source": [
    "# For logging\n",
    "train_reward_lst = []\n",
    "eval_reward_mean_lst = []\n",
    "eval_reward_var_lst = []\n",
    "\n",
    "\n",
    "# Set up environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "state_size = env.observation_space.shape[0] # Number of observations (CartPole)\n",
    "action_size = env.action_space.n            # Number of possible actions\n",
    "\n",
    "model_dir = \"q_net_baseline\"  # TensorBoard log directory\n",
    "cb = keras.callbacks.TensorBoard(log_dir = get_run_logdir(model_dir), histogram_freq=1)\n",
    "\n",
    "# For timing training\n",
    "total_training_time = 0\n",
    "\n",
    "\n",
    "for ep in range(episode):\n",
    "    state, _ = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    total_reward = 0\n",
    "\n",
    "    # record start time\n",
    "    start = time.time()\n",
    "\n",
    "    for _ in range(500):\n",
    "        # Interact with the environment with epsilon-greedy policy\n",
    "        if np.random.rand() <= epsilon:\n",
    "            action = np.random.choice(action_size)\n",
    "        else:\n",
    "            pass # remove pass and use 2 lines below\n",
    "            # q_values = model.predict(state, verbose=0)\n",
    "            # action = np.argmax(q_values)\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # Train model using Q-Learning update:  Q(s, a) = r + gamma * max Q(s', a')\n",
    "        # [WriteCode]\n",
    "\n",
    "        # Hints:\n",
    "\n",
    "        # 1. Compute target Q-values:\n",
    "        # - If done, Q-target = reward (no future reward)\n",
    "        # - Otherwise, Q-target = reward + gamma * max(Q(next_state, a))\n",
    "\n",
    "        # 2. Predict current Q-values for state\n",
    "        # Update only the Q-value for the taken action\n",
    "\n",
    "        # 3. Fit the model:\n",
    "        # - Inputs: state\n",
    "        # - Targets: updated Q-values (with action Q-value replaced by computed target)\n",
    "\n",
    "\n",
    "        # Update exploration rate\n",
    "        if epsilon > epsilon_min:\n",
    "            epsilon *= epsilon_decay\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # record end time and log the training time\n",
    "    end = time.time()\n",
    "    total_training_time += end - start\n",
    "\n",
    "    # Evaluation\n",
    "    # [WriteCode]\n",
    "\n",
    " #   print(f\"Episode {ep + 1}/{episode} | Ep. Total Reward: {total_reward}\"\n",
    " #       f\" | Epsilon : {epsilon:.3f}\"\n",
    " #       f\" | Eval Rwd Mean: {eval_reward_mean:.2f}\"\n",
    " #       f\" | Eval Rwd Var: {eval_reward_var:.2f}\")\n",
    "\n",
    "    # Log\n",
    "#    eval_reward_mean_lst.append(eval_reward_mean)\n",
    "#    eval_reward_var_lst.append(eval_reward_var)\n",
    "#    train_reward_lst.append(total_reward)\n",
    "\n",
    "    # Early Stopping Condition to avoid overfitting\n",
    "    # If the evaluation reward reaches the specified threshold, stop training early.\n",
    "    # The default threshold is set to 500, but you should adjust this based on observed training performance.\n",
    "#    if eval_reward_mean > 500: # [Modify this threshold as needed]\n",
    "#        print(f\"Early stopping triggered at Episode {ep + 1}.\")\n",
    "#        break\n",
    "\n",
    "# evaluate average training time per episode\n",
    "print(f\"Training time: {total_training_time/(ep + 1):.4f} seconds per episode\")\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qZ86BPQR3AhY"
   },
   "source": [
    "## Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "It8u9fyj3AL5"
   },
   "outputs": [],
   "source": [
    "# Write code to plot\n",
    "# 1) Moving Averaged Training Reward, 2) Evaluation Mean, 3) Evaluation Variance\n",
    "# [Write Code]\n",
    "\n",
    "# plot_smoothed_training_rwd(...\n",
    "\n",
    "# plot_eval_rwd_mean(...\n",
    "\n",
    "# plot_eval_rwd_var(..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSlOTDlpm78g"
   },
   "source": [
    "# Q-Network - Alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "c9zO7SZssiaR"
   },
   "outputs": [],
   "source": [
    "# [Write Code]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lg9kBHn2slYF"
   },
   "source": [
    "## Comparison\n",
    "\n",
    "Compare performance of the Baseline and the Alternative Naive Q-Network policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "E6DiE3YRsm2k"
   },
   "outputs": [],
   "source": [
    "# [Write Code]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dhgXEg3097Ud"
   },
   "source": [
    "# Experience Replay Framework\n",
    "\n",
    "By now, you may have noticed that in a standard RL setup, the agent **only learns from its most recent experience** at each step. This makes training unstable and inefficient. Additionally, consecutive experiences are often highly correlated, which can lead to poor generalization.\n",
    "\n",
    "To address this, we will adopt the **Experience Replay Framework**, where past interactions are stored in a **Replay Buffer** and **a randomly sampled batch** is used to update the decision-making policy. This helps to:\n",
    "\n",
    "- **Break correlation** between consecutive experiences, stabilizing learning.\n",
    "- **Improve data efficiency** by reusing past experiences multiple times.\n",
    "- **Smooth training updates**, preventing abrupt policy changes.\n",
    "\n",
    "The figure below illustrates this process:\n",
    "![dqn-agent.png](assets/dqn-agent.png)\n",
    "\n",
    "- The Agent interacts with the Environment, taking actions based on its learned policy.\n",
    "- The Replay Buffer stores past experiences `(state, action, reward, next state, done)`.\n",
    "- Instead of updating after every step, the agent samples from the buffer and learns from past interactions.\n",
    "\n",
    "This framework is used in naive DQN, DQN and DDQN to stabilize Q-learning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CkfcXDjNriXH"
   },
   "source": [
    "## Implement Replay Buffer and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "lJH9KigXsX8Y"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Define replay buffer\\nreplay_buffer = deque(maxlen=10000)\\n\\n# Store experience tuple into replay_buffer\\nstore_experience(replay_buffer, state, action, reward, next_state, done)\\n\\n# Sample mini-batch of size ba from replay_buffer\\nstates, actions, rewards, next_states, dones = sample_experience(replay_buffer, ba)\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store (state, action, reward, next_state, done)\n",
    "def store_experience(replay_buffer, state, action, reward, next_stat, done):\n",
    "        replay_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "# Sample (state, action, reward, next_state, done) mini-batch for training\n",
    "def sample_experience(replay_buffer, batch_size):\n",
    "\n",
    "    # Ensure we have enough samples\n",
    "    assert len(replay_buffer) >= batch_size, (\n",
    "        f\"Not enough samples in buffer to sample {batch_size} items.\")\n",
    "\n",
    "    # Sample a mini-batch\n",
    "    minibatch = random.sample(replay_buffer, batch_size)\n",
    "\n",
    "    states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "\n",
    "    states = np.array(states, dtype=np.float32).squeeze()\n",
    "    next_states = np.array(next_states, dtype=np.float32).squeeze()\n",
    "    actions = np.array(actions, dtype=np.int32)\n",
    "    rewards = np.array(rewards, dtype=np.float32)\n",
    "    dones = np.array(dones, dtype=np.float32)\n",
    "\n",
    "    return states, actions, rewards, next_states, dones\n",
    "\n",
    "# [Hint] You may find the following statements useful\n",
    "\"\"\"\n",
    "# Define replay buffer\n",
    "replay_buffer = deque(maxlen=10000)\n",
    "\n",
    "# Store experience tuple into replay_buffer\n",
    "store_experience(replay_buffer, state, action, reward, next_state, done)\n",
    "\n",
    "# Sample mini-batch of size ba from replay_buffer\n",
    "states, actions, rewards, next_states, dones = sample_experience(replay_buffer, ba)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OCkBN3gIiLFP"
   },
   "source": [
    "# Naive DQN - Baseline\n",
    "\n",
    "In this section, you will complete the Naive DQN alorithm by implementing the missing components marked as `[WriteCode]`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tyRVFFdXvFYk"
   },
   "source": [
    "## Define and Compile the Neural Network\n",
    "\n",
    "A single network $Q_\\theta$ (parameterized by $\\theta$) is used to approximate $Q(s,a)$.\n",
    "\n",
    "The target used by Naive DQN is then:\n",
    "\n",
    "$Y^{NaiveQ}_t = R_{t+1} + \\gamma Q_{\\theta}(S_{t+1}, a)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "83PVW-l7DRc6"
   },
   "outputs": [],
   "source": [
    "# Naive DQN Baseline Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "\n",
    "# [WriteCode] from ... import ...\n",
    "\n",
    "# Define the Q-network\n",
    "model = Sequential()\n",
    "\n",
    "# [WriteCode]\n",
    "# model.add(...\n",
    "\n",
    "# Compile the model\n",
    "# [WriteCode]\n",
    "\n",
    "# Print the model summary\n",
    "# [WriteCode]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9X6H46OIExH"
   },
   "source": [
    "## Set Up Env and Train the Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "wK70zqKbIHWJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/250 | Ep. Total Reward: 11.0 | Epsilon : 0.007\n",
      "Episode 2/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 3/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 4/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 5/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 6/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 7/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 8/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 9/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 10/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 11/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 12/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 13/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 14/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 15/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 16/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 17/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 18/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 19/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 20/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 21/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 22/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 23/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 24/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 25/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 26/250 | Ep. Total Reward: 21.0 | Epsilon : 0.007\n",
      "Episode 27/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 28/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 29/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 30/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 31/250 | Ep. Total Reward: 25.0 | Epsilon : 0.007\n",
      "Episode 32/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 33/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 34/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 35/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 36/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 37/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 38/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 39/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 40/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 41/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 42/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 43/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 44/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 45/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 46/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 47/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 48/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 49/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 50/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 51/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 52/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 53/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 54/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 55/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 56/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 57/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 58/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 59/250 | Ep. Total Reward: 11.0 | Epsilon : 0.007\n",
      "Episode 60/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 61/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 62/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 63/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 64/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 65/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 66/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 67/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 68/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 69/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 70/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 71/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 72/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 73/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 74/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 75/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 76/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 77/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 78/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 79/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 80/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 81/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 82/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 83/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 84/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 85/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 86/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 87/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 88/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 89/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 90/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 91/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 92/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 93/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 94/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 95/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 96/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 97/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 98/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 99/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 100/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 101/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 102/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 103/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 104/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 105/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 106/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 107/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 108/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 109/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 110/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 111/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 112/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 113/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 114/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 115/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 116/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 117/250 | Ep. Total Reward: 11.0 | Epsilon : 0.007\n",
      "Episode 118/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 119/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 120/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 121/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 122/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 123/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 124/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 125/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 126/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 127/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 128/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 129/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 130/250 | Ep. Total Reward: 11.0 | Epsilon : 0.007\n",
      "Episode 131/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 132/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 133/250 | Ep. Total Reward: 15.0 | Epsilon : 0.007\n",
      "Episode 134/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 135/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 136/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 137/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 138/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 139/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 140/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 141/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 142/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 143/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 144/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 145/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 146/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 147/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 148/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 149/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 150/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 151/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 152/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 153/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 154/250 | Ep. Total Reward: 11.0 | Epsilon : 0.007\n",
      "Episode 155/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 156/250 | Ep. Total Reward: 19.0 | Epsilon : 0.007\n",
      "Episode 157/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 158/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 159/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 160/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 161/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 162/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 163/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 164/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 165/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 166/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 167/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 168/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 169/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 170/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 171/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 172/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 173/250 | Ep. Total Reward: 11.0 | Epsilon : 0.007\n",
      "Episode 174/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 175/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 176/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 177/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 178/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 179/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 180/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 181/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 182/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 183/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 184/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 185/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 186/250 | Ep. Total Reward: 11.0 | Epsilon : 0.007\n",
      "Episode 187/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 188/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 189/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 190/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 191/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 192/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 193/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 194/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 195/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 196/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 197/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 198/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 199/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 200/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 201/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 202/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 203/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 204/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 205/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 206/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 207/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 208/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 209/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 210/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 211/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 212/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 213/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 214/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 215/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 216/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 217/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 218/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 219/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 220/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 221/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 222/250 | Ep. Total Reward: 11.0 | Epsilon : 0.007\n",
      "Episode 223/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 224/250 | Ep. Total Reward: 11.0 | Epsilon : 0.007\n",
      "Episode 225/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 226/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 227/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 228/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 229/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 230/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 231/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 232/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 233/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 234/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 235/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 236/250 | Ep. Total Reward: 11.0 | Epsilon : 0.007\n",
      "Episode 237/250 | Ep. Total Reward: 11.0 | Epsilon : 0.007\n",
      "Episode 238/250 | Ep. Total Reward: 11.0 | Epsilon : 0.007\n",
      "Episode 239/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 240/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 241/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 242/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 243/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 244/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 245/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 246/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 247/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 248/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 249/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 250/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Training time: 0.0000 seconds per episode\n"
     ]
    }
   ],
   "source": [
    "# For logging\n",
    "train_reward_lst = []\n",
    "eval_reward_mean_lst = []\n",
    "eval_reward_var_lst = []\n",
    "\n",
    "# Set up environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "state_size = env.observation_space.shape[0] # Number of observations (CartPole)\n",
    "action_size = env.action_space.n            # Number of possible actions\n",
    "\n",
    "model_dir = \"naive_dqn_baseline\"  # TensorBoard log directory\n",
    "cb = keras.callbacks.TensorBoard(log_dir = get_run_logdir(model_dir), histogram_freq=1)\n",
    "\n",
    "# For timing training\n",
    "total_training_time = 0\n",
    "\n",
    "# Define replay buffer\n",
    "# [WriteCode]\n",
    "# replay_buffer = ...\n",
    "\n",
    "for ep in range(episode):\n",
    "    state, _ = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    total_reward = 0\n",
    "\n",
    "    # record start time\n",
    "    start = time.time()\n",
    "\n",
    "    for _ in range(500):\n",
    "        # Interact with the environment with epsilon-greedy policy\n",
    "        if np.random.rand() <= epsilon:\n",
    "            action = np.random.choice(action_size)\n",
    "        else:\n",
    "            pass # remove pass and use 2 lines below\n",
    "#            q_values = model.predict(state, verbose=0)\n",
    "#            action = np.argmax(q_values)\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # store experience into replay buffer\n",
    "        # [WriteCode]\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "\n",
    "#        if len(replay_buffer) >= ba:\n",
    "            # Update policy with mini-batches if replay buffer contains enough samples\n",
    "            # Update policy using Q-Learning update:  Q(s, a) = r + gamma * max Q(s', a')\n",
    "            # [WriteCode]\n",
    "\n",
    "            # Hints:\n",
    "\n",
    "            # 1. Compute target Q-values:\n",
    "            # - If done, Q-target = reward (no future reward)\n",
    "            # - Otherwise, Q-target = reward + gamma * max(Q(next_state, a))\n",
    "\n",
    "            # 2. Predict current Q-values for state\n",
    "            # Update only the Q-value for the taken action\n",
    "\n",
    "            # 3. Fit the model:\n",
    "            # - Inputs: state\n",
    "            # - Targets: updated Q-values (with action Q-value replaced by computed target)\n",
    "\n",
    "            # Update exploration rate\n",
    "            if epsilon > epsilon_min:\n",
    "                epsilon *= epsilon_decay\n",
    "\n",
    "    # record end time and log training time\n",
    "    end = time.time()\n",
    "    total_training_time += end - start\n",
    "\n",
    "    # Evaluation\n",
    "    # [WriteCode]\n",
    "\n",
    "\n",
    "    print(f\"Episode {ep + 1}/{episode} | Ep. Total Reward: {total_reward}\"\n",
    "        f\" | Epsilon : {epsilon:.3f}\")\n",
    "#        f\" | Eval Rwd Mean: {eval_reward_mean:.2f}\"\n",
    "#        f\" | Eval Rwd Var: {eval_reward_var:.2f}\")\n",
    "\n",
    "    # Log\n",
    "#    eval_reward_mean_lst.append(eval_reward_mean)\n",
    "#    eval_reward_var_lst.append(eval_reward_var)\n",
    "#    train_reward_lst.append(total_reward)\n",
    "\n",
    "    # Early Stopping Condition to avoid overfitting\n",
    "    # If the evaluation reward reaches the specified threshold, stop training early.\n",
    "    # The default threshold is set to 500, but you should adjust this based on observed training performance.\n",
    "#    if eval_reward_mean > 500: # [Modify this threshold as needed]\n",
    "#        print(f\"Early stopping triggered at Episode {ep + 1}.\")\n",
    "#        break\n",
    "\n",
    "# evaluate average training time per episode\n",
    "print(f\"Training time: {total_training_time/episode:.4f} seconds per episode\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMgwUOjQ30iI"
   },
   "source": [
    "## Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "FcQkJXSl30iI"
   },
   "outputs": [],
   "source": [
    "# Write code to plot\n",
    "# 1) Moving Averaged Training Reward, 2) Evaluation Mean, 3) Evaluation Variance\n",
    "# [Write Code]\n",
    "\n",
    "# plot_smoothed_training_rwd(...\n",
    "\n",
    "# plot_eval_rwd_mean(...\n",
    "\n",
    "# plot_eval_rwd_var(...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qsx1mQTZwhNC"
   },
   "source": [
    "# Naive DQN - Alternative\n",
    "You may insert extra cells in the notebook to perform tuning experiments and log results effectively. Use TensorBoard, plots, or tables to visualize the impact of different hyperparameter choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "auXumGG5w7JN"
   },
   "outputs": [],
   "source": [
    "# [Write Code]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lAiI3y2yw47Q"
   },
   "source": [
    "## Comparison\n",
    "\n",
    "Compare performance of the Baseline and the Alternative Naive DQN Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "dHIjFgUhw-aR"
   },
   "outputs": [],
   "source": [
    "# [Write Code]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iy8REuq6iQah"
   },
   "source": [
    "# DQN\n",
    "\n",
    "In this section, you will complete the DQN algorithm by implementing the missing components marked as `[WriteCode]`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXqZUwXkxYci"
   },
   "source": [
    "## Define and Compile the Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2FvxFyz5hnEP"
   },
   "source": [
    "The standard DQN [https://arxiv.org/abs/1312.5602] approach features a periodically updated target network to stabilize training:\n",
    "\n",
    "- `target_model`: A periodically updated network that stabilizes training. During training, this network is used to compute $Q(S_{t+1}, a)$ for each possible action $a$. This network is not actively trained, but at regular intervals, its weights are copied from `eval_model`. This prevents rapid fluctuations in target Q-values and improves learning stability.\n",
    "\n",
    "- `eval_model`: The online learning network that interacts with the environment. During training, this network is used to compute $Q(S_{t}, a)$ for each possible action $a$. It updates its weights by minimizing the difference between predicted and target Q-values, improving the agent's decision-making over time.\n",
    "\n",
    "The target used by DQN is then:\n",
    "\n",
    "$Y^{Q}_t = R_{t+1} + \\gamma \\max_a Q_{target}(S_{t+1}, a)$\n",
    "\n",
    "In the following section, define `[eval_model]` and `[target_model]` with identical architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "NYILkt0Gzd5G"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# DQN Baseline Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "\n",
    "# [WriteCode] from ... import ...\n",
    "\n",
    "# Define the eval (online) network\n",
    "eval_model = Sequential()\n",
    "\n",
    "# [WriteCode]\n",
    "# model.add(...\n",
    "\n",
    "# Compile the model\n",
    "# [WriteCode]\n",
    "\n",
    "# Print the model summary\n",
    "# [WriteCode]\n",
    "\n",
    "\n",
    "# Create target_model with the same architecture\n",
    "target_model = Sequential()\n",
    "\n",
    "# [WriteCode]\n",
    "# model.add(...\n",
    "\n",
    "# Skip compiling as target_model will not be trained with .fit()\n",
    "# Instead, weights will be copied from the online model\n",
    "target_model.set_weights(eval_model.get_weights())\n",
    "\n",
    "# Print the model summary\n",
    "# [WriteCode]\n",
    "print(target_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZfiKQHk_Vjb"
   },
   "source": [
    "## Set Up Env and Train the Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hsudLCVWB1Em"
   },
   "source": [
    "In this section, you will complete the DQN training routine by implementing the missing components marked as `[WriteCode]`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "I96clO81hFJ_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 2/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 3/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 4/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 5/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 6/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 7/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 8/250 | Ep. Total Reward: 12.0 | Epsilon : 0.007\n",
      "Episode 9/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 10/250 | Ep. Total Reward: 24.0 | Epsilon : 0.007\n",
      "Episode 11/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 12/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 13/250 | Ep. Total Reward: 11.0 | Epsilon : 0.007\n",
      "Episode 14/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 15/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 16/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 17/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 18/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 19/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 20/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 21/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 22/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 23/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 24/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 25/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 26/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 27/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 28/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 29/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 30/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 31/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 32/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 33/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 34/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 35/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 36/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 37/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 38/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 39/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 40/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 41/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 42/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 43/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 44/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 45/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 46/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 47/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 48/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 49/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 50/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 51/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 52/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 53/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 54/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 55/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 56/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 57/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 58/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 59/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 60/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 61/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 62/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 63/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 64/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 65/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 66/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 67/250 | Ep. Total Reward: 15.0 | Epsilon : 0.007\n",
      "Episode 68/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 69/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 70/250 | Ep. Total Reward: 11.0 | Epsilon : 0.007\n",
      "Episode 71/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 72/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 73/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 74/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 75/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 76/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 77/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 78/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 79/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 80/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 81/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 82/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 83/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 84/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 85/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 86/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 87/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 88/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 89/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 90/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 91/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 92/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 93/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 94/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 95/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 96/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 97/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 98/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 99/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 100/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 101/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 102/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 103/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 104/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 105/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 106/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 107/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 108/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 109/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 110/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 111/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 112/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 113/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 114/250 | Ep. Total Reward: 11.0 | Epsilon : 0.007\n",
      "Episode 115/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 116/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 117/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 118/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 119/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 120/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 121/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 122/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 123/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 124/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 125/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 126/250 | Ep. Total Reward: 17.0 | Epsilon : 0.007\n",
      "Episode 127/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 128/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 129/250 | Ep. Total Reward: 11.0 | Epsilon : 0.007\n",
      "Episode 130/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 131/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 132/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 133/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 134/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 135/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 136/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 137/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 138/250 | Ep. Total Reward: 22.0 | Epsilon : 0.007\n",
      "Episode 139/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 140/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 141/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 142/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 143/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 144/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 145/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 146/250 | Ep. Total Reward: 11.0 | Epsilon : 0.007\n",
      "Episode 147/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 148/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 149/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 150/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 151/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 152/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 153/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 154/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 155/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 156/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 157/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 158/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 159/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 160/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 161/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 162/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 163/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 164/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 165/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 166/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 167/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 168/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 169/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 170/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 171/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 172/250 | Ep. Total Reward: 11.0 | Epsilon : 0.007\n",
      "Episode 173/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 174/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 175/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 176/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 177/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 178/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 179/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 180/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 181/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 182/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 183/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 184/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 185/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 186/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 187/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 188/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 189/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 190/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 191/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 192/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 193/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 194/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 195/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 196/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 197/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 198/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 199/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 200/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 201/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 202/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 203/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 204/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 205/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 206/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 207/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 208/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 209/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 210/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 211/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 212/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 213/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 214/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 215/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 216/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 217/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 218/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 219/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 220/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 221/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 222/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 223/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 224/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 225/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 226/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 227/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 228/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 229/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 230/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 231/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 232/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 233/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 234/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 235/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 236/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 237/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 238/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 239/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 240/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 241/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 242/250 | Ep. Total Reward: 8.0 | Epsilon : 0.007\n",
      "Episode 243/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 244/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 245/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 246/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 247/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 248/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Episode 249/250 | Ep. Total Reward: 10.0 | Epsilon : 0.007\n",
      "Episode 250/250 | Ep. Total Reward: 9.0 | Epsilon : 0.007\n",
      "Training time: 0.0000 seconds per episode\n"
     ]
    }
   ],
   "source": [
    "# For logging\n",
    "train_reward_lst = []\n",
    "eval_reward_mean_lst = []\n",
    "eval_reward_var_lst = []\n",
    "\n",
    "# Set up environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "state_size = env.observation_space.shape[0] # Number of observations (CartPole)\n",
    "action_size = env.action_space.n            # Number of possible actions\n",
    "\n",
    "model_dir = \"dqn_baseline\"   # TensorBoard log directory\n",
    "cb = keras.callbacks.TensorBoard(log_dir = get_run_logdir(model_dir), histogram_freq=1)\n",
    "\n",
    "# Train Counter for weight syncing\n",
    "train_counter = 0\n",
    "\n",
    "# For timing training\n",
    "total_training_time = 0\n",
    "\n",
    "# Define replay buffer\n",
    "# replay_buffer = ...\n",
    "\n",
    "for ep in range(episode):\n",
    "    state, _ = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    total_reward = 0\n",
    "\n",
    "    # record start time\n",
    "    start = time.time()\n",
    "\n",
    "    for _ in range(500):\n",
    "        # Interact with the environment with epsilon-greedy policy\n",
    "        if np.random.rand() <= epsilon:\n",
    "            action = np.random.choice(action_size)\n",
    "        else:\n",
    "            pass # remove pass and 2 lines below\n",
    "#            q_values = eval_model.predict(state, verbose=0)\n",
    "#            action = np.argmax(q_values)\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # store experience into replay buffer\n",
    "        # [WriteCode]\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        # if len(replay_buffer) >= ba:\n",
    "            # train_counter += 1\n",
    "            # Update policy with mini-batches if replay buffer contains enough samples\n",
    "            # Update policy using Deep Q-Learning update: Q(s, a) = r + gamma * max Q_target(S', a)\n",
    "            # [WriteCode]\n",
    "\n",
    "            # Compute target Q-values:\n",
    "            # - If done, Q-target = reward (no future reward)\n",
    "            # - Otherwise, Q-target = reward + gamma * max(Q_target(next_state, a))\n",
    "\n",
    "            # Predict current Q-values for state using eval_model\n",
    "            # Predict future Q-values using target_model (NOT eval_model)\n",
    "\n",
    "            # Update only the Q-value for the taken action\n",
    "\n",
    "            # Fit the model:\n",
    "            # - Inputs: state\n",
    "            # - Targets: updated Q-values (with action Q-value replaced by computed target)\n",
    "\n",
    "\n",
    "            # Update exploration rate\n",
    "            if epsilon > epsilon_min:\n",
    "                epsilon *= epsilon_decay\n",
    "\n",
    "            # Periodically update the target network\n",
    "            if train_counter % target_update_freq == 0:\n",
    "                target_model.set_weights(eval_model.get_weights())\n",
    "\n",
    "    # record end time and log training time\n",
    "    end = time.time()\n",
    "    total_training_time += end - start\n",
    "\n",
    "    # Evaluation\n",
    "    # [WriteCode]\n",
    "\n",
    "    print(f\"Episode {ep + 1}/{episode} | Ep. Total Reward: {total_reward}\"\n",
    "        f\" | Epsilon : {epsilon:.3f}\")\n",
    "#        f\" | Eval Rwd Mean: {eval_reward_mean:.2f}\"\n",
    "#        f\" | Eval Rwd Var: {eval_reward_var:.2f}\")\n",
    "\n",
    "    # Log\n",
    "#    eval_reward_mean_lst.append(eval_reward_mean)\n",
    "#    eval_reward_var_lst.append(eval_reward_var)\n",
    "#    train_reward_lst.append(total_reward)\n",
    "\n",
    "    # Early Stopping Condition to avoid overfitting\n",
    "    # If the evaluation reward reaches the specified threshold, stop training early.\n",
    "    # The default threshold is set to 500, but you should adjust this based on observed training performance.\n",
    "#    if eval_reward_mean > 500: # [Modify this threshold as needed]\n",
    "#        print(f\"Early stopping triggered at Episode {ep + 1}.\")\n",
    "#        break\n",
    "\n",
    "# evaluate average training time per episode\n",
    "print(f\"Training time: {total_training_time/episode:.4f} seconds per episode\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cWTB-X8Q35lX"
   },
   "source": [
    "## Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "gUC2aelO35lX"
   },
   "outputs": [],
   "source": [
    "# Write code to plot\n",
    "# 1) Moving Averaged Training Reward, 2) Evaluation Mean, 3) Evaluation Variance\n",
    "# [Write Code]\n",
    "plot_smoothed_training_rwd(train_reward_lst, window_size=20)\n",
    "plot_eval_rwd_mean(eval_reward_mean_lst)\n",
    "plot_eval_rwd_var(eval_reward_var_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aGOFWup5ZiYp"
   },
   "source": [
    "# DQN - Alternative\n",
    "\n",
    "You may insert extra cells in the notebook to perform tuning experiments and log results effectively. Use TensorBoard, plots, or tables to visualize the impact of different hyperparameter choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eOwN55lA34nm"
   },
   "source": [
    "## Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "hf5220Bw34nm"
   },
   "outputs": [],
   "source": [
    "# Write code to plot\n",
    "# 1) Moving Averaged Training Reward, 2) Evaluation Mean, 3) Evaluation Variance\n",
    "# [Write Code]\n",
    "\n",
    "# plot_smoothed_training_rwd(...\n",
    "\n",
    "# plot_eval_rwd_mean(...\n",
    "\n",
    "# plot_eval_rwd_var(..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "SvxOyxpPZlfW"
   },
   "outputs": [],
   "source": [
    "# [Write Code]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0oTzmTnlZlru"
   },
   "source": [
    "## Comparison\n",
    "\n",
    "Compare performance of the Baseline and the Alternative DQN Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "HiMW7bE1Zq2B"
   },
   "outputs": [],
   "source": [
    "# [Write Code]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4yGOGzD5iRw5"
   },
   "source": [
    "# DDQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-nURxQ5zZwu5"
   },
   "source": [
    "## Define and Compile the Neural Networks\n",
    "\n",
    "The Double Deep Q-Network (DDQN) [https://arxiv.org/abs/1509.06461] improves upon standard DQN by reducing overestimation bias in Q-values. DDQN achieves this by decoupling action selection from value estimation using two networks, which is almost identical as DQN:\n",
    "\n",
    "- `target_model`: A periodically updated network that stabilizes training. During training, this network is used to compute $Q(S_{t+1}, a)$ for each possible action $a$. This network is also not trained but synced from `eval_model`. **However, unlike DQN, this network is only used to evaluate the Q-value of the action chosen by `eval_model`, making value updates more stable.**\n",
    "\n",
    "- `eval_model`: The online learning network that interacts with the environment. During training, this network is used to select the best action using $\\arg\\max_a Q(S_{t}, a)$ and updates its weights by minimizing the difference between predicted and target Q-value.\n",
    "\n",
    "The target used by DDQN is then:\n",
    "\n",
    "$Y^{Q}_t = R_{t+1} + \\gamma Q_{target}(S_{t+1}, \\arg \\max_{a} Q_{eval}(S_{t+1}, a))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Env "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, np.int64(2))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "state_size = env.observation_space.shape[0] # Number of observations (CartPole)\n",
    "action_size = env.action_space.n            # Number of possible actions\n",
    "state_size, action_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "NoH_Tjmz3FBn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_model:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">110</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │            \u001b[38;5;34m50\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │            \u001b[38;5;34m40\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m110\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │            \u001b[38;5;34m40\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │            \u001b[38;5;34m22\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">262</span> (1.02 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m262\u001b[0m (1.02 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">222</span> (888.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m222\u001b[0m (888.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">40</span> (160.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m40\u001b[0m (160.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_model(planb):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">420</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │           \u001b[38;5;34m100\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │            \u001b[38;5;34m80\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │           \u001b[38;5;34m420\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │            \u001b[38;5;34m80\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │            \u001b[38;5;34m42\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">722</span> (2.82 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m722\u001b[0m (2.82 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">642</span> (2.51 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m642\u001b[0m (2.51 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">80</span> (320.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m80\u001b[0m (320.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_model:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_6\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_6\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">110</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │            \u001b[38;5;34m50\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │            \u001b[38;5;34m40\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m110\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │            \u001b[38;5;34m40\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │            \u001b[38;5;34m22\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">262</span> (1.02 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m262\u001b[0m (1.02 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">222</span> (888.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m222\u001b[0m (888.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">40</span> (160.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m40\u001b[0m (160.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_model(planb):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_7\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_7\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">420</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │           \u001b[38;5;34m100\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │            \u001b[38;5;34m80\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │           \u001b[38;5;34m420\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │            \u001b[38;5;34m80\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │            \u001b[38;5;34m42\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">722</span> (2.82 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m722\u001b[0m (2.82 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">642</span> (2.51 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m642\u001b[0m (2.51 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">80</span> (320.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m80\u001b[0m (320.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DDQN Baseline Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.losses import SparseCategoricalCrossentropy, MeanSquaredError\n",
    "\n",
    "# [WriteCode] from ... import ...\n",
    "from policy.ddqn import DoubleDeepQNetworkPolicy\n",
    "from model.ddqn import DoubleDeepQNetworkTagetModel, DoubleDeepQNetworkEvalModel\n",
    "from config.ddqn_cfg import DDQNConfig\n",
    "\n",
    "# Define the eval (online) network\n",
    "eval_model = DoubleDeepQNetworkEvalModel(\n",
    "    DDQNConfig(\n",
    "    input_dim=state_size,\n",
    "    output_dim=action_size,\n",
    "    use_bias=True,\n",
    "    is_compiled=True,\n",
    "    hidden_dims=[10, 10],\n",
    "    optimizer='adam',\n",
    "    loss=MeanSquaredError(),\n",
    "    metrics=['accuracy'])\n",
    ")\n",
    "print('eval_model:')\n",
    "eval_model.summary()\n",
    "eval_model_planb = DoubleDeepQNetworkEvalModel(\n",
    "    DDQNConfig(\n",
    "    input_dim=state_size,\n",
    "    output_dim=action_size,\n",
    "    use_bias=True,\n",
    "    hidden_dims=[20, 20],\n",
    "    is_compiled=True,\n",
    "    optimizer='adam',\n",
    "    loss=MeanSquaredError(),\n",
    "    metrics=['accuracy'])\n",
    ")\n",
    "print('eval_model(planb):')\n",
    "eval_model_planb.summary()\n",
    "\n",
    "# Create target_model with the same architecture\n",
    "target_model = DoubleDeepQNetworkTagetModel(\n",
    "    DDQNConfig(\n",
    "    input_dim=state_size,\n",
    "    output_dim=action_size,\n",
    "    use_bias=True,\n",
    "    is_compiled=False,\n",
    "    hidden_dims=[10, 10],\n",
    "    optimizer='adam',\n",
    "    loss=MeanSquaredError(),\n",
    "    metrics=['accuracy'])\n",
    ")\n",
    "print('target_model:')\n",
    "target_model.summary()\n",
    "\n",
    "target_model_planb = DoubleDeepQNetworkTagetModel(\n",
    "    DDQNConfig(\n",
    "    input_dim=state_size,\n",
    "    output_dim=action_size,\n",
    "    use_bias=True,\n",
    "    is_compiled=False,\n",
    "    hidden_dims=[20, 20],\n",
    "    optimizer='adam',\n",
    "    loss=MeanSquaredError(),\n",
    "    metrics=['accuracy'])\n",
    ")\n",
    "print('target_model(planb):')\n",
    "target_model_planb.summary()\n",
    "# target_model = Sequential()\n",
    "\n",
    "# [WriteCode]\n",
    "# model.add(...\n",
    "\n",
    "# Skip compiling as target_model will not be trained with .fit()\n",
    "# Instead, weights will be copied from the online model\n",
    "target_model.set_weights(eval_model.get_weights())\n",
    "target_model_planb.set_weights(eval_model_planb.get_weights())\n",
    "# Print the model summary\n",
    "# [WriteCode]\n",
    "\n",
    "xqy_policy = DoubleDeepQNetworkPolicy(\n",
    "    target_model=target_model,\n",
    "    eval_model=eval_model\n",
    ")\n",
    "xqy_policy_planb = DoubleDeepQNetworkPolicy(\n",
    "    target_model=target_model_planb,\n",
    "    eval_model=eval_model_planb\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-uZT2NevZzo7"
   },
   "source": [
    "## Train the Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 384ms/step - accuracy: 0.4688 - loss: 14.0390\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4375 - loss: 15.9953\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.3750 - loss: 17.4606\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.3125 - loss: 19.9551\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.4375 - loss: 12.8471\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.5312 - loss: 20.6820\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5000 - loss: 15.9313\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5312 - loss: 13.8038\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.4688 - loss: 14.4479\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5000 - loss: 27.7570\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5312 - loss: 14.2539\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5625 - loss: 18.7803\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.5938 - loss: 22.5597\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6250 - loss: 16.4373\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5938 - loss: 16.2890\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6562 - loss: 29.9725\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5000 - loss: 14.6948\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5312 - loss: 17.2785\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5312 - loss: 13.9637\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6875 - loss: 20.9304\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6562 - loss: 19.2965\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5312 - loss: 17.7209\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5938 - loss: 19.4585\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5625 - loss: 18.9058\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5312 - loss: 17.7749\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5938 - loss: 14.8124\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5625 - loss: 17.5022\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5312 - loss: 15.8184\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5312 - loss: 17.8764\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.5312 - loss: 14.6896\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5000 - loss: 13.0955\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5938 - loss: 14.8035\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5938 - loss: 16.0353\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6875 - loss: 24.5247\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5625 - loss: 15.5226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5938 - loss: 15.9988\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6875 - loss: 16.3672\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7812 - loss: 19.3801\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5625 - loss: 16.6587\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5625 - loss: 22.7257\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6875 - loss: 22.8098\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8125 - loss: 17.0282\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7812 - loss: 15.9047\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5938 - loss: 19.9088\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5625 - loss: 13.5432\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6250 - loss: 12.7982\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6562 - loss: 13.0582\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6562 - loss: 20.4003\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6562 - loss: 14.1235\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.6562 - loss: 17.7520\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7812 - loss: 19.6735\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7500 - loss: 24.8917\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6250 - loss: 14.0033\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5938 - loss: 12.0713\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6875 - loss: 14.3262\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6875 - loss: 29.3649\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6562 - loss: 15.8435\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5625 - loss: 19.6254\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.5625 - loss: 13.0478\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6250 - loss: 18.7177\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6250 - loss: 14.0614\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.6250 - loss: 17.4419\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9062 - loss: 22.9622\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8125 - loss: 13.5273\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6562 - loss: 16.2757\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6562 - loss: 13.2079\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8438 - loss: 15.0156\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8438 - loss: 18.3252\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7500 - loss: 16.6138\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9375 - loss: 18.3824\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.6875 - loss: 16.0899\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5938 - loss: 19.6361\n",
      "Training time: 0.7107 seconds per episode\n"
     ]
    }
   ],
   "source": [
    "from trainner import Trainer\n",
    "ddqn_trainner = Trainer(\n",
    "    model_dir=\"ddqn_baseline\",\n",
    "    get_run_logdir=get_run_logdir,\n",
    ")\n",
    "ddqn_trainner.train(xqy_policy, train_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oKfbQWJq363X"
   },
   "source": [
    "## Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L7qIOwiX363Y"
   },
   "outputs": [],
   "source": [
    "# Write code to plot\n",
    "# 1) Moving Averaged Training Reward, 2) Evaluation Mean, 3) Evaluation Variance\n",
    "# [Write Code]\n",
    "\n",
    "# plot_smoothed_training_rwd(...\n",
    "\n",
    "# plot_eval_rwd_mean(...\n",
    "\n",
    "# plot_eval_rwd_var(...\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from visualization import GraphPloter\n",
    "\n",
    "gp = GraphPloter()\n",
    "def plot_eval_rwd_mean(eval_mean_list):\n",
    "    \"\"\"Plot evaluation reward mean.\"\"\"\n",
    "    # gp.plot_eval_rwd_mean(eval_mean_list)\n",
    "\n",
    "def plot_eval_rwd_var(eval_var_list):\n",
    "    \"\"\"Plot evaluation reward variance.\"\"\"\n",
    "    # gp.plot_eval_rwd_var(eval_var_list)\n",
    "\n",
    "\n",
    "def plot_smoothed_training_rwd(train_rwd_list, window_size=20):\n",
    "    \"\"\"Plot smoothed training rewards using a moving average.\"\"\"\n",
    "    # gp.plot_smoothed_training_rwd(train_rwd_list, window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YgVEqsvQHsiD"
   },
   "source": [
    "# DDQN - Alternative\n",
    "\n",
    "You may insert extra cells in the notebook to perform tuning experiments and log results effectively. Use TensorBoard, plots, or tables to visualize the impact of different hyperparameter choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "0xno9cGCHrx5"
   },
   "outputs": [],
   "source": [
    "# [Write Code]\n",
    "from visualization import Comparator\n",
    "# xqy_comparator = Comparator(base_policy, xqy_policy)\n",
    "# xqy_comparator.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5MBGp2LfHxDr"
   },
   "source": [
    "## Comparison\n",
    "\n",
    "Compare performance of the Baseline and the Alternative DDQN Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "3NDTwhmwH3Lk"
   },
   "outputs": [],
   "source": [
    "# [Write Code]\n",
    "from visualization import Comparator\n",
    "xqy_comparator = Comparator(xqy_policy, xqy_policy_planb)\n",
    "xqy_comparator.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUo4vcwQnbWV"
   },
   "source": [
    "# Visualize with Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "ZcIJ3DQc3-Jw"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-f0310e76da5f6a96\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-f0310e76da5f6a96\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./eec4400_logs --port=6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-insiT7D_lq"
   },
   "source": [
    "# Comparison Across Four Alternative Policies\n",
    "\n",
    "Compare hyperparameters and performance of the four alternative policies.\n",
    "\n",
    "You may insert extra cells in the notebook to tabulate/plot/log results effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "SjQwsaxSE5Jh"
   },
   "outputs": [],
   "source": [
    "# [Write Code]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "tfdlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
