{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gBUF2HJd1fVT"
   },
   "outputs": [],
   "source": [
    "Student_1 = '' #@param {type:\"string\"}  Name of student 1\n",
    "Student_2 = '' #@param {type:\"string\"}  Name of student 2\n",
    "Student_3 = 'HE, Yunhan' #@param {type:\"string\"}  Name of student 3\n",
    "Student_4 = 'XIE, Qingyun' #@param {type:\"string\"}  Name of student 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9PhWOSD2hLUM"
   },
   "source": [
    "# Overview\n",
    "\n",
    "This is the skeleton code file for the EEC4400 assignment. Replace the `XX`. in the name of this Jupyter notebook with your group number (this is important for correct marks to be awarded to your group). Fill in the blank cells below with the necessary code (you should work on this Jupyter notebook section by section). At the end, the entire Jupyter notebook should generate all the required results and execute without error.\n",
    "\n",
    "The text above the blank cells provides information on the functionality that needs to be implemented. You need to write Python code at places indicated by `[WriteCode]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jXckzF3Fhd4p"
   },
   "source": [
    "## Install and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E669iRUFhn9J"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9d3w4FRMS36_"
   },
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8RD1Ggwcqg1"
   },
   "source": [
    "## Environment Exploration: Cart-Pole\n",
    "\n",
    "The **Cart-Pole environment** is a classic reinforcement learning problem where a pole is attached by an un-actuated joint to a cart. The cart moves along a frictionless track, and the goal is to balance the pole upright by applying forces to move the cart left or right.\n",
    "\n",
    "<img src=\"https://gymnasium.farama.org/_images/cart_pole.gif\" width=\"400\">\n",
    "\n",
    "Below is a breakdown of the base Cart-Pole environment:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geWhWkTJRu5w"
   },
   "source": [
    "#### - **Observation Space (*s*)**\n",
    "The observation is represented as a `ndarray` of shape `(4,)`, corresponding to:\n",
    "- **Cart Position**: Horizontal location of the cart.\n",
    "- **Cart Velocity**: Speed of the cart along the track.\n",
    "- **Pole Angle**: Angular position of the pole relative to vertical.\n",
    "- **Pole Angular Velocity**: Speed at which the pole angle changes.\n",
    "\n",
    "The observations are initialized with uniformly random values in the range `(-0.05, 0.05)`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8r3befVJR4SJ"
   },
   "source": [
    "#### - **Action Space (*a*)**\n",
    "The action is a discrete value (`0` or `1`) indicating the direction of the force applied to the cart:\n",
    "- `0`: Push the cart to the left.\n",
    "- `1`: Push the cart to the right.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BfanxRL4R5jO"
   },
   "source": [
    "\n",
    "#### - **Reward Function (*r*)**\n",
    "The agent receives a reward of `+1` for each time step it successfully keeps the pole upright. The maximum achievable reward in a single episode is `500`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "spv7Ul4CR7SN"
   },
   "source": [
    "#### - **Episode Termination (*isDone*)**\n",
    "An episode ends if any of the following conditions occur:\n",
    "1. **Pole Angle** exceeds `±12°`.\n",
    "2. **Cart Position** exceeds `±2.4` (the cart reaches the edge of the track).\n",
    "3. **Truncation**: The episode reaches the maximum length of `500 steps`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pD50TBeLif6F"
   },
   "source": [
    "## Getting Familiar with Basic Gymnasium Usage\n",
    "\n",
    "Let's observe how to interact with a Gymnasium environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kkeWe780XZVv"
   },
   "source": [
    "The classic “agent-environment loop” pictured below is a simplified representation of reinforcement learning that Gymnasium implements.\n",
    "\n",
    "![agt-env.png](assets/agt-env.png)\n",
    "\n",
    "The following section represents a simple episode of this loop, we will:\n",
    "\n",
    "1. Import the base Cart-Pole environment from the gymnasium library and explore its functionality.\n",
    "\n",
    "2. Apply modifications to make the environment more challenging by adjusting the pole's dynamics and introducing stochastic forces.\n",
    "\n",
    "3. Sample random actions to control the cart-pole.\n",
    "\n",
    "These steps will help us understand how the environment behaves under different conditions and prepare us for implementing reinforcement learning agents in later sections.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x2PMYpIKXtWv"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import and Explore the Environment\n",
    "print(\"\\n--- Exploring the Base Environment ---\")\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "observation, info = env.reset()\n",
    "\n",
    "# Print information about the environment\n",
    "print(f\"Initial Observation: {observation}\")\n",
    "print(f\"Action Space: {env.action_space}\")  # Discrete actions: 0 (left), 1 (right)\n",
    "print(f\"Observation Space: {env.observation_space}\")  # State variables: cart position, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9wjDE0kfFJd"
   },
   "source": [
    "# Define Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0UFVPRylCdv"
   },
   "source": [
    "## Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vsmmRtAVlUz_"
   },
   "outputs": [],
   "source": [
    "# Some plotting functions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_eval_rwd_mean(eval_mean_list):\n",
    "    \"\"\"Plot evaluation reward mean.\"\"\"\n",
    "    # [WriteCode]\n",
    "\n",
    "\n",
    "\n",
    "def plot_eval_rwd_var(eval_var_list):\n",
    "    \"\"\"Plot evaluation reward variance.\"\"\"\n",
    "    # [WriteCode]\n",
    "\n",
    "\n",
    "\n",
    "def plot_smoothed_training_rwd(train_rwd_list, window_size=20):\n",
    "    \"\"\"Plot smoothed training rewards using a moving average.\"\"\"\n",
    "    # [WriteCode]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N7hA67eaYkCW"
   },
   "source": [
    "## Evaluation Function\n",
    "\n",
    "Evaluate the learnt policy by running 5 evaluation episodes and computing the average and variance of rewards collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kKS1vpSmYjyg"
   },
   "outputs": [],
   "source": [
    "def evaluation(model, max_timesteps=500):\n",
    "    eval_env = gym.make(\"CartPole-v1\")\n",
    "    state_size = eval_env.observation_space.shape[0] # Number of observations (CartPole)\n",
    "    action_size = eval_env.action_space.n            # Number of possible actions\n",
    "    eval_reward = []\n",
    "\n",
    "    for i in range (5):\n",
    "        round_reward = 0\n",
    "        state, _ = eval_env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "\n",
    "        for i in range(max_timesteps):\n",
    "            action = np.argmax(model.predict(state, verbose=0)[0])\n",
    "            next_state, reward, terminated, truncated, _ = eval_env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "\n",
    "            round_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "            if terminated or truncated:\n",
    "                eval_reward.append(round_reward)\n",
    "                break\n",
    "\n",
    "    eval_env.close()\n",
    "\n",
    "    eval_reward_mean = np.sum(eval_reward)/len(eval_reward)\n",
    "    eval_reward_var = np.var(eval_reward)\n",
    "\n",
    "    return eval_reward_mean, eval_reward_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kWS8Kd8eiXau"
   },
   "source": [
    "# Setting up Tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ADO90Q7Hs-lx"
   },
   "outputs": [],
   "source": [
    "def get_run_logdir(k):\n",
    "    root_logdir = os.path.join(os.curdir, \"eec4400_logs\", k)\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B07kopgXtOVU"
   },
   "source": [
    "# Initialize Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JL9ABa8-vRf3"
   },
   "outputs": [],
   "source": [
    "# Use the following set of NN hyperparameters for ALL FOUR baseline policies\n",
    "lr =  1e-4        #@param {type:\"number\"}               # learning rate\n",
    "epoch =  1     #@param {type:\"number\"}               # epochs\n",
    "episode = 250  #@param {type:\"number\"}               # episodes\n",
    "\n",
    "epsilon = 1           #@param {type:\"number\"}     # Starting exploration rate\n",
    "epsilon_min = 0.01    #@param {type:\"number\"}     # Exploration rate min\n",
    "epsilon_decay = 0     #@param {type:\"number\"}     # Exploration rate decay\n",
    "\n",
    "gamma = 0.99          #@param {type:\"number\"}     # Agent discount factor\n",
    "\n",
    "# Use the following set of NN hyperparameters for Naive DQN, DQN and DDQN policies\n",
    "ba =  32       #@param {type:\"number\"}               # batch_size\n",
    "\n",
    "# Use the following set of RL hyperparameters for DQN and DDQN policies\n",
    "target_update_freq = 0 # @param {type:\"number\"}    # Target network update frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.train_cfg import TrainningConfig\n",
    "train_cfg = TrainningConfig(\n",
    "    lr=lr,\n",
    "    epoch=epoch,\n",
    "    episode=episode,\n",
    "    epsilon=epsilon,\n",
    "    epsilon_min=epsilon_min,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    gamma=gamma,\n",
    "    ba=ba,\n",
    "    target_update_freq=target_update_freq\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "npUYex5Em46O"
   },
   "source": [
    "# Q-Network - Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLr4PDQ7nC5T"
   },
   "source": [
    "## Define and Compile the Neural Network\n",
    "\n",
    "A single network $Q_\\theta$ (parameterized by $\\theta$) is used to approximate $Q(s,a)$.\n",
    "\n",
    "The target used by Naive DQN is then:\n",
    "\n",
    "$Y^{NaiveQ}_t = R_{t+1} + \\gamma Q_{\\theta}(S_{t+1}, a)$\n",
    "\n",
    "The training of Q-Network does NOT rely on a Replay Buffer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WyXho4UKpH3Q"
   },
   "outputs": [],
   "source": [
    "# Q-Network Baseline Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "\n",
    "# [WriteCode] from ... import ...\n",
    "\n",
    "\n",
    "# Define the Q-network\n",
    "model = Sequential()\n",
    "\n",
    "# [WriteCode]\n",
    "# model.add(...\n",
    "\n",
    "# Compile the model\n",
    "# [WriteCode]\n",
    "\n",
    "# Print the model summary\n",
    "# [WriteCode]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PunGDA41nK-t"
   },
   "source": [
    "## Set Up Env and Train the Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-cDAJtaGnCQL"
   },
   "outputs": [],
   "source": [
    "# For logging\n",
    "train_reward_lst = []\n",
    "eval_reward_mean_lst = []\n",
    "eval_reward_var_lst = []\n",
    "\n",
    "\n",
    "# Set up environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "state_size = env.observation_space.shape[0] # Number of observations (CartPole)\n",
    "action_size = env.action_space.n            # Number of possible actions\n",
    "\n",
    "model_dir = \"q_net_baseline\"  # TensorBoard log directory\n",
    "cb = keras.callbacks.TensorBoard(log_dir = get_run_logdir(model_dir), histogram_freq=1)\n",
    "\n",
    "# For timing training\n",
    "total_training_time = 0\n",
    "\n",
    "\n",
    "for ep in range(episode):\n",
    "    state, _ = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    total_reward = 0\n",
    "\n",
    "    # record start time\n",
    "    start = time.time()\n",
    "\n",
    "    for _ in range(500):\n",
    "        # Interact with the environment with epsilon-greedy policy\n",
    "        if np.random.rand() <= epsilon:\n",
    "            action = np.random.choice(action_size)\n",
    "        else:\n",
    "            pass # remove pass and use 2 lines below\n",
    "            # q_values = model.predict(state, verbose=0)\n",
    "            # action = np.argmax(q_values)\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # Train model using Q-Learning update:  Q(s, a) = r + gamma * max Q(s', a')\n",
    "        # [WriteCode]\n",
    "\n",
    "        # Hints:\n",
    "\n",
    "        # 1. Compute target Q-values:\n",
    "        # - If done, Q-target = reward (no future reward)\n",
    "        # - Otherwise, Q-target = reward + gamma * max(Q(next_state, a))\n",
    "\n",
    "        # 2. Predict current Q-values for state\n",
    "        # Update only the Q-value for the taken action\n",
    "\n",
    "        # 3. Fit the model:\n",
    "        # - Inputs: state\n",
    "        # - Targets: updated Q-values (with action Q-value replaced by computed target)\n",
    "\n",
    "\n",
    "        # Update exploration rate\n",
    "        if epsilon > epsilon_min:\n",
    "            epsilon *= epsilon_decay\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # record end time and log the training time\n",
    "    end = time.time()\n",
    "    total_training_time += end - start\n",
    "\n",
    "    # Evaluation\n",
    "    # [WriteCode]\n",
    "\n",
    " #   print(f\"Episode {ep + 1}/{episode} | Ep. Total Reward: {total_reward}\"\n",
    " #       f\" | Epsilon : {epsilon:.3f}\"\n",
    " #       f\" | Eval Rwd Mean: {eval_reward_mean:.2f}\"\n",
    " #       f\" | Eval Rwd Var: {eval_reward_var:.2f}\")\n",
    "\n",
    "    # Log\n",
    "#    eval_reward_mean_lst.append(eval_reward_mean)\n",
    "#    eval_reward_var_lst.append(eval_reward_var)\n",
    "#    train_reward_lst.append(total_reward)\n",
    "\n",
    "    # Early Stopping Condition to avoid overfitting\n",
    "    # If the evaluation reward reaches the specified threshold, stop training early.\n",
    "    # The default threshold is set to 500, but you should adjust this based on observed training performance.\n",
    "#    if eval_reward_mean > 500: # [Modify this threshold as needed]\n",
    "#        print(f\"Early stopping triggered at Episode {ep + 1}.\")\n",
    "#        break\n",
    "\n",
    "# evaluate average training time per episode\n",
    "print(f\"Training time: {total_training_time/(ep + 1):.4f} seconds per episode\")\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qZ86BPQR3AhY"
   },
   "source": [
    "## Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "It8u9fyj3AL5"
   },
   "outputs": [],
   "source": [
    "# Write code to plot\n",
    "# 1) Moving Averaged Training Reward, 2) Evaluation Mean, 3) Evaluation Variance\n",
    "# [Write Code]\n",
    "\n",
    "# plot_smoothed_training_rwd(...\n",
    "\n",
    "# plot_eval_rwd_mean(...\n",
    "\n",
    "# plot_eval_rwd_var(..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSlOTDlpm78g"
   },
   "source": [
    "# Q-Network - Alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c9zO7SZssiaR"
   },
   "outputs": [],
   "source": [
    "# [Write Code]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lg9kBHn2slYF"
   },
   "source": [
    "## Comparison\n",
    "\n",
    "Compare performance of the Baseline and the Alternative Naive Q-Network policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E6DiE3YRsm2k"
   },
   "outputs": [],
   "source": [
    "# [Write Code]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dhgXEg3097Ud"
   },
   "source": [
    "# Experience Replay Framework\n",
    "\n",
    "By now, you may have noticed that in a standard RL setup, the agent **only learns from its most recent experience** at each step. This makes training unstable and inefficient. Additionally, consecutive experiences are often highly correlated, which can lead to poor generalization.\n",
    "\n",
    "To address this, we will adopt the **Experience Replay Framework**, where past interactions are stored in a **Replay Buffer** and **a randomly sampled batch** is used to update the decision-making policy. This helps to:\n",
    "\n",
    "- **Break correlation** between consecutive experiences, stabilizing learning.\n",
    "- **Improve data efficiency** by reusing past experiences multiple times.\n",
    "- **Smooth training updates**, preventing abrupt policy changes.\n",
    "\n",
    "The figure below illustrates this process:\n",
    "![dqn-agent.png](assets/dqn-agent.png)\n",
    "\n",
    "- The Agent interacts with the Environment, taking actions based on its learned policy.\n",
    "- The Replay Buffer stores past experiences `(state, action, reward, next state, done)`.\n",
    "- Instead of updating after every step, the agent samples from the buffer and learns from past interactions.\n",
    "\n",
    "This framework is used in naive DQN, DQN and DDQN to stabilize Q-learning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CkfcXDjNriXH"
   },
   "source": [
    "## Implement Replay Buffer and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lJH9KigXsX8Y"
   },
   "outputs": [],
   "source": [
    "# Store (state, action, reward, next_state, done)\n",
    "def store_experience(replay_buffer, state, action, reward, next_stat, done):\n",
    "        replay_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "# Sample (state, action, reward, next_state, done) mini-batch for training\n",
    "def sample_experience(replay_buffer, batch_size):\n",
    "\n",
    "    # Ensure we have enough samples\n",
    "    assert len(replay_buffer) >= batch_size, (\n",
    "        f\"Not enough samples in buffer to sample {batch_size} items.\")\n",
    "\n",
    "    # Sample a mini-batch\n",
    "    minibatch = random.sample(replay_buffer, batch_size)\n",
    "\n",
    "    states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "\n",
    "    states = np.array(states, dtype=np.float32).squeeze()\n",
    "    next_states = np.array(next_states, dtype=np.float32).squeeze()\n",
    "    actions = np.array(actions, dtype=np.int32)\n",
    "    rewards = np.array(rewards, dtype=np.float32)\n",
    "    dones = np.array(dones, dtype=np.float32)\n",
    "\n",
    "    return states, actions, rewards, next_states, dones\n",
    "\n",
    "# [Hint] You may find the following statements useful\n",
    "\"\"\"\n",
    "# Define replay buffer\n",
    "replay_buffer = deque(maxlen=10000)\n",
    "\n",
    "# Store experience tuple into replay_buffer\n",
    "store_experience(replay_buffer, state, action, reward, next_state, done)\n",
    "\n",
    "# Sample mini-batch of size ba from replay_buffer\n",
    "states, actions, rewards, next_states, dones = sample_experience(replay_buffer, ba)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OCkBN3gIiLFP"
   },
   "source": [
    "# Naive DQN - Baseline\n",
    "\n",
    "In this section, you will complete the Naive DQN alorithm by implementing the missing components marked as `[WriteCode]`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tyRVFFdXvFYk"
   },
   "source": [
    "## Define and Compile the Neural Network\n",
    "\n",
    "A single network $Q_\\theta$ (parameterized by $\\theta$) is used to approximate $Q(s,a)$.\n",
    "\n",
    "The target used by Naive DQN is then:\n",
    "\n",
    "$Y^{NaiveQ}_t = R_{t+1} + \\gamma Q_{\\theta}(S_{t+1}, a)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "83PVW-l7DRc6"
   },
   "outputs": [],
   "source": [
    "# Naive DQN Baseline Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "\n",
    "# [WriteCode] from ... import ...\n",
    "\n",
    "# Define the Q-network\n",
    "model = Sequential()\n",
    "\n",
    "# [WriteCode]\n",
    "# model.add(...\n",
    "\n",
    "# Compile the model\n",
    "# [WriteCode]\n",
    "\n",
    "# Print the model summary\n",
    "# [WriteCode]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9X6H46OIExH"
   },
   "source": [
    "## Set Up Env and Train the Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wK70zqKbIHWJ"
   },
   "outputs": [],
   "source": [
    "# For logging\n",
    "train_reward_lst = []\n",
    "eval_reward_mean_lst = []\n",
    "eval_reward_var_lst = []\n",
    "\n",
    "# Set up environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "state_size = env.observation_space.shape[0] # Number of observations (CartPole)\n",
    "action_size = env.action_space.n            # Number of possible actions\n",
    "\n",
    "model_dir = \"naive_dqn_baseline\"  # TensorBoard log directory\n",
    "cb = keras.callbacks.TensorBoard(log_dir = get_run_logdir(model_dir), histogram_freq=1)\n",
    "\n",
    "# For timing training\n",
    "total_training_time = 0\n",
    "\n",
    "# Define replay buffer\n",
    "# [WriteCode]\n",
    "# replay_buffer = ...\n",
    "\n",
    "for ep in range(episode):\n",
    "    state, _ = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    total_reward = 0\n",
    "\n",
    "    # record start time\n",
    "    start = time.time()\n",
    "\n",
    "    for _ in range(500):\n",
    "        # Interact with the environment with epsilon-greedy policy\n",
    "        if np.random.rand() <= epsilon:\n",
    "            action = np.random.choice(action_size)\n",
    "        else:\n",
    "            pass # remove pass and use 2 lines below\n",
    "#            q_values = model.predict(state, verbose=0)\n",
    "#            action = np.argmax(q_values)\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # store experience into replay buffer\n",
    "        # [WriteCode]\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "\n",
    "#        if len(replay_buffer) >= ba:\n",
    "            # Update policy with mini-batches if replay buffer contains enough samples\n",
    "            # Update policy using Q-Learning update:  Q(s, a) = r + gamma * max Q(s', a')\n",
    "            # [WriteCode]\n",
    "\n",
    "            # Hints:\n",
    "\n",
    "            # 1. Compute target Q-values:\n",
    "            # - If done, Q-target = reward (no future reward)\n",
    "            # - Otherwise, Q-target = reward + gamma * max(Q(next_state, a))\n",
    "\n",
    "            # 2. Predict current Q-values for state\n",
    "            # Update only the Q-value for the taken action\n",
    "\n",
    "            # 3. Fit the model:\n",
    "            # - Inputs: state\n",
    "            # - Targets: updated Q-values (with action Q-value replaced by computed target)\n",
    "\n",
    "            # Update exploration rate\n",
    "            if epsilon > epsilon_min:\n",
    "                epsilon *= epsilon_decay\n",
    "\n",
    "    # record end time and log training time\n",
    "    end = time.time()\n",
    "    total_training_time += end - start\n",
    "\n",
    "    # Evaluation\n",
    "    # [WriteCode]\n",
    "\n",
    "\n",
    "    print(f\"Episode {ep + 1}/{episode} | Ep. Total Reward: {total_reward}\"\n",
    "        f\" | Epsilon : {epsilon:.3f}\")\n",
    "#        f\" | Eval Rwd Mean: {eval_reward_mean:.2f}\"\n",
    "#        f\" | Eval Rwd Var: {eval_reward_var:.2f}\")\n",
    "\n",
    "    # Log\n",
    "#    eval_reward_mean_lst.append(eval_reward_mean)\n",
    "#    eval_reward_var_lst.append(eval_reward_var)\n",
    "#    train_reward_lst.append(total_reward)\n",
    "\n",
    "    # Early Stopping Condition to avoid overfitting\n",
    "    # If the evaluation reward reaches the specified threshold, stop training early.\n",
    "    # The default threshold is set to 500, but you should adjust this based on observed training performance.\n",
    "#    if eval_reward_mean > 500: # [Modify this threshold as needed]\n",
    "#        print(f\"Early stopping triggered at Episode {ep + 1}.\")\n",
    "#        break\n",
    "\n",
    "# evaluate average training time per episode\n",
    "print(f\"Training time: {total_training_time/episode:.4f} seconds per episode\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMgwUOjQ30iI"
   },
   "source": [
    "## Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FcQkJXSl30iI"
   },
   "outputs": [],
   "source": [
    "# Write code to plot\n",
    "# 1) Moving Averaged Training Reward, 2) Evaluation Mean, 3) Evaluation Variance\n",
    "# [Write Code]\n",
    "\n",
    "# plot_smoothed_training_rwd(...\n",
    "\n",
    "# plot_eval_rwd_mean(...\n",
    "\n",
    "# plot_eval_rwd_var(...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qsx1mQTZwhNC"
   },
   "source": [
    "# Naive DQN - Alternative\n",
    "You may insert extra cells in the notebook to perform tuning experiments and log results effectively. Use TensorBoard, plots, or tables to visualize the impact of different hyperparameter choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "auXumGG5w7JN"
   },
   "outputs": [],
   "source": [
    "# [Write Code]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lAiI3y2yw47Q"
   },
   "source": [
    "## Comparison\n",
    "\n",
    "Compare performance of the Baseline and the Alternative Naive DQN Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dHIjFgUhw-aR"
   },
   "outputs": [],
   "source": [
    "# [Write Code]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iy8REuq6iQah"
   },
   "source": [
    "# DQN\n",
    "\n",
    "In this section, you will complete the DQN algorithm by implementing the missing components marked as `[WriteCode]`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXqZUwXkxYci"
   },
   "source": [
    "## Define and Compile the Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2FvxFyz5hnEP"
   },
   "source": [
    "The standard DQN [https://arxiv.org/abs/1312.5602] approach features a periodically updated target network to stabilize training:\n",
    "\n",
    "- `target_model`: A periodically updated network that stabilizes training. During training, this network is used to compute $Q(S_{t+1}, a)$ for each possible action $a$. This network is not actively trained, but at regular intervals, its weights are copied from `eval_model`. This prevents rapid fluctuations in target Q-values and improves learning stability.\n",
    "\n",
    "- `eval_model`: The online learning network that interacts with the environment. During training, this network is used to compute $Q(S_{t}, a)$ for each possible action $a$. It updates its weights by minimizing the difference between predicted and target Q-values, improving the agent's decision-making over time.\n",
    "\n",
    "The target used by DQN is then:\n",
    "\n",
    "$Y^{Q}_t = R_{t+1} + \\gamma \\max_a Q_{target}(S_{t+1}, a)$\n",
    "\n",
    "In the following section, define `[eval_model]` and `[target_model]` with identical architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NYILkt0Gzd5G"
   },
   "outputs": [],
   "source": [
    "# DQN Baseline Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "\n",
    "# [WriteCode] from ... import ...\n",
    "\n",
    "# Define the eval (online) network\n",
    "eval_model = Sequential()\n",
    "\n",
    "# [WriteCode]\n",
    "# model.add(...\n",
    "\n",
    "# Compile the model\n",
    "# [WriteCode]\n",
    "\n",
    "# Print the model summary\n",
    "# [WriteCode]\n",
    "\n",
    "\n",
    "# Create target_model with the same architecture\n",
    "target_model = Sequential()\n",
    "\n",
    "# [WriteCode]\n",
    "# model.add(...\n",
    "\n",
    "# Skip compiling as target_model will not be trained with .fit()\n",
    "# Instead, weights will be copied from the online model\n",
    "target_model.set_weights(eval_model.get_weights())\n",
    "\n",
    "# Print the model summary\n",
    "# [WriteCode]\n",
    "print(target_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZfiKQHk_Vjb"
   },
   "source": [
    "## Set Up Env and Train the Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hsudLCVWB1Em"
   },
   "source": [
    "In this section, you will complete the DQN training routine by implementing the missing components marked as `[WriteCode]`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I96clO81hFJ_"
   },
   "outputs": [],
   "source": [
    "# For logging\n",
    "train_reward_lst = []\n",
    "eval_reward_mean_lst = []\n",
    "eval_reward_var_lst = []\n",
    "\n",
    "# Set up environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "state_size = env.observation_space.shape[0] # Number of observations (CartPole)\n",
    "action_size = env.action_space.n            # Number of possible actions\n",
    "\n",
    "model_dir = \"dqn_baseline\"   # TensorBoard log directory\n",
    "cb = keras.callbacks.TensorBoard(log_dir = get_run_logdir(model_dir), histogram_freq=1)\n",
    "\n",
    "# Train Counter for weight syncing\n",
    "train_counter = 0\n",
    "\n",
    "# For timing training\n",
    "total_training_time = 0\n",
    "\n",
    "# Define replay buffer\n",
    "# replay_buffer = ...\n",
    "\n",
    "for ep in range(episode):\n",
    "    state, _ = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    total_reward = 0\n",
    "\n",
    "    # record start time\n",
    "    start = time.time()\n",
    "\n",
    "    for _ in range(500):\n",
    "        # Interact with the environment with epsilon-greedy policy\n",
    "        if np.random.rand() <= epsilon:\n",
    "            action = np.random.choice(action_size)\n",
    "        else:\n",
    "            pass # remove pass and 2 lines below\n",
    "#            q_values = eval_model.predict(state, verbose=0)\n",
    "#            action = np.argmax(q_values)\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # store experience into replay buffer\n",
    "        # [WriteCode]\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        # if len(replay_buffer) >= ba:\n",
    "            # train_counter += 1\n",
    "            # Update policy with mini-batches if replay buffer contains enough samples\n",
    "            # Update policy using Deep Q-Learning update: Q(s, a) = r + gamma * max Q_target(S', a)\n",
    "            # [WriteCode]\n",
    "\n",
    "            # Compute target Q-values:\n",
    "            # - If done, Q-target = reward (no future reward)\n",
    "            # - Otherwise, Q-target = reward + gamma * max(Q_target(next_state, a))\n",
    "\n",
    "            # Predict current Q-values for state using eval_model\n",
    "            # Predict future Q-values using target_model (NOT eval_model)\n",
    "\n",
    "            # Update only the Q-value for the taken action\n",
    "\n",
    "            # Fit the model:\n",
    "            # - Inputs: state\n",
    "            # - Targets: updated Q-values (with action Q-value replaced by computed target)\n",
    "\n",
    "\n",
    "            # Update exploration rate\n",
    "            if epsilon > epsilon_min:\n",
    "                epsilon *= epsilon_decay\n",
    "\n",
    "            # Periodically update the target network\n",
    "            if train_counter % target_update_freq == 0:\n",
    "                target_model.set_weights(eval_model.get_weights())\n",
    "\n",
    "    # record end time and log training time\n",
    "    end = time.time()\n",
    "    total_training_time += end - start\n",
    "\n",
    "    # Evaluation\n",
    "    # [WriteCode]\n",
    "\n",
    "    print(f\"Episode {ep + 1}/{episode} | Ep. Total Reward: {total_reward}\"\n",
    "        f\" | Epsilon : {epsilon:.3f}\")\n",
    "#        f\" | Eval Rwd Mean: {eval_reward_mean:.2f}\"\n",
    "#        f\" | Eval Rwd Var: {eval_reward_var:.2f}\")\n",
    "\n",
    "    # Log\n",
    "#    eval_reward_mean_lst.append(eval_reward_mean)\n",
    "#    eval_reward_var_lst.append(eval_reward_var)\n",
    "#    train_reward_lst.append(total_reward)\n",
    "\n",
    "    # Early Stopping Condition to avoid overfitting\n",
    "    # If the evaluation reward reaches the specified threshold, stop training early.\n",
    "    # The default threshold is set to 500, but you should adjust this based on observed training performance.\n",
    "#    if eval_reward_mean > 500: # [Modify this threshold as needed]\n",
    "#        print(f\"Early stopping triggered at Episode {ep + 1}.\")\n",
    "#        break\n",
    "\n",
    "# evaluate average training time per episode\n",
    "print(f\"Training time: {total_training_time/episode:.4f} seconds per episode\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cWTB-X8Q35lX"
   },
   "source": [
    "## Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gUC2aelO35lX"
   },
   "outputs": [],
   "source": [
    "# Write code to plot\n",
    "# 1) Moving Averaged Training Reward, 2) Evaluation Mean, 3) Evaluation Variance\n",
    "# [Write Code]\n",
    "plot_smoothed_training_rwd(train_reward_lst, window_size=20)\n",
    "plot_eval_rwd_mean(eval_reward_mean_lst)\n",
    "plot_eval_rwd_var(eval_reward_var_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aGOFWup5ZiYp"
   },
   "source": [
    "# DQN - Alternative\n",
    "\n",
    "You may insert extra cells in the notebook to perform tuning experiments and log results effectively. Use TensorBoard, plots, or tables to visualize the impact of different hyperparameter choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eOwN55lA34nm"
   },
   "source": [
    "## Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hf5220Bw34nm"
   },
   "outputs": [],
   "source": [
    "# Write code to plot\n",
    "# 1) Moving Averaged Training Reward, 2) Evaluation Mean, 3) Evaluation Variance\n",
    "# [Write Code]\n",
    "\n",
    "# plot_smoothed_training_rwd(...\n",
    "\n",
    "# plot_eval_rwd_mean(...\n",
    "\n",
    "# plot_eval_rwd_var(..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SvxOyxpPZlfW"
   },
   "outputs": [],
   "source": [
    "# [Write Code]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0oTzmTnlZlru"
   },
   "source": [
    "## Comparison\n",
    "\n",
    "Compare performance of the Baseline and the Alternative DQN Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HiMW7bE1Zq2B"
   },
   "outputs": [],
   "source": [
    "# [Write Code]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4yGOGzD5iRw5"
   },
   "source": [
    "# DDQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-nURxQ5zZwu5"
   },
   "source": [
    "## Define and Compile the Neural Networks\n",
    "\n",
    "The Double Deep Q-Network (DDQN) [https://arxiv.org/abs/1509.06461] improves upon standard DQN by reducing overestimation bias in Q-values. DDQN achieves this by decoupling action selection from value estimation using two networks, which is almost identical as DQN:\n",
    "\n",
    "- `target_model`: A periodically updated network that stabilizes training. During training, this network is used to compute $Q(S_{t+1}, a)$ for each possible action $a$. This network is also not trained but synced from `eval_model`. **However, unlike DQN, this network is only used to evaluate the Q-value of the action chosen by `eval_model`, making value updates more stable.**\n",
    "\n",
    "- `eval_model`: The online learning network that interacts with the environment. During training, this network is used to select the best action using $\\arg\\max_a Q(S_{t}, a)$ and updates its weights by minimizing the difference between predicted and target Q-value.\n",
    "\n",
    "The target used by DDQN is then:\n",
    "\n",
    "$Y^{Q}_t = R_{t+1} + \\gamma Q_{target}(S_{t+1}, \\arg \\max_{a} Q_{eval}(S_{t+1}, a))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NoH_Tjmz3FBn"
   },
   "outputs": [],
   "source": [
    "# DDQN Baseline Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "\n",
    "# [WriteCode] from ... import ...\n",
    "from policy.ddqn import DoubleDeepQNetworkPolicy\n",
    "from model.ddqn import DoubleDeepQNetworkTagetModel, DoubleDeepQNetworkEvalModel\n",
    "from config.ddqn_cfg import DDQNConfig\n",
    "ddqn_cfg = DDQNConfig()\n",
    "ddqn_cfg_planb = DDQNConfig()\n",
    "\n",
    "# Define the eval (online) network\n",
    "eval_model = DoubleDeepQNetworkEvalModel(ddqn_cfg)\n",
    "eval_model_planb = DoubleDeepQNetworkEvalModel(ddqn_cfg_planb)\n",
    "# eval_model = Sequential()\n",
    "\n",
    "# [WriteCode]\n",
    "# model.add(...\n",
    "\n",
    "# Compile the model\n",
    "# [WriteCode]\n",
    "\n",
    "# Print the model summary\n",
    "# [WriteCode]\n",
    "\n",
    "\n",
    "\n",
    "# Create target_model with the same architecture\n",
    "target_model = DoubleDeepQNetworkTagetModel(ddqn_cfg)\n",
    "target_model_planb = DoubleDeepQNetworkTagetModel(ddqn_cfg_planb)\n",
    "# target_model = Sequential()\n",
    "\n",
    "# [WriteCode]\n",
    "# model.add(...\n",
    "\n",
    "# Skip compiling as target_model will not be trained with .fit()\n",
    "# Instead, weights will be copied from the online model\n",
    "target_model.set_weights(eval_model.get_weights())\n",
    "target_model_planb.set_weights(eval_model_planb.get_weights())\n",
    "# Print the model summary\n",
    "# [WriteCode]\n",
    "\n",
    "xqy_policy = DoubleDeepQNetworkPolicy(\n",
    "    target_model=target_model,\n",
    "    eval_model=eval_model\n",
    ")\n",
    "xqy_policy_planb = DoubleDeepQNetworkPolicy(\n",
    "    target_model=target_model_planb,\n",
    "    eval_model=eval_model_planb\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-uZT2NevZzo7"
   },
   "source": [
    "## Set Up Env and Train the Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainner import Trainer\n",
    "trainner = Trainer(\n",
    "    policy=xqy_policy,\n",
    "    model_dir=\"ddqn_baseline\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4U9ntMqg3TH7"
   },
   "outputs": [],
   "source": [
    "# For logging\n",
    "train_reward_lst = []\n",
    "eval_reward_mean_lst = []\n",
    "eval_reward_var_lst = []\n",
    "\n",
    "# Set up environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "state_size = env.observation_space.shape[0] # Number of observations (CartPole)\n",
    "action_size = env.action_space.n            # Number of possible actions\n",
    "\n",
    "model_dir = \"ddqn_baseline\"  # TensorBoard log directory\n",
    "cb = keras.callbacks.TensorBoard(log_dir = get_run_logdir(model_dir), histogram_freq=1)\n",
    "\n",
    "# Train Counter for weight syncing\n",
    "train_counter = 0\n",
    "\n",
    "# For timing training\n",
    "total_training_time = 0\n",
    "\n",
    "# Define replay buffer\n",
    "# replay_buffer = ...\n",
    "xqy_policy.init_buffer()\n",
    "\n",
    "for ep in range(episode):\n",
    "    state, _ = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    total_reward = 0\n",
    "\n",
    "    # record start time\n",
    "    start = time.time()\n",
    "\n",
    "    for _ in range(500):\n",
    "        # Interact with the environment with epsilon-greedy policy\n",
    "        xqy_policy.act(state, action_size, epsilon)\n",
    "        # if np.random.rand() <= epsilon:\n",
    "        #     action = np.random.choice(action_size)\n",
    "        # else:\n",
    "        #     pass # remove pass and use 2 lines below\n",
    "        #     # q_values = eval_model.predict(state, verbose=0)\n",
    "        #     # action = np.argmax(q_values)\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # store experience into replay buffer\n",
    "        # [WriteCode]\n",
    "        xqy_policy.store_experience(state, action, reward, next_state, done)\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        xqy_policy.step()\n",
    "        # if len(replay_buffer) >= ba:\n",
    "        #   train_counter += 1\n",
    "            # Update policy with mini-batches if replay buffer contains enough samples\n",
    "            # Update policy using Double Deep Q-Learning update:\n",
    "            # Q(s, a) = r + gamma * Q_target(S', argmax Q_eval(S', a))\n",
    "            # [WriteCode]\n",
    "\n",
    "            # Compute target Q-values:\n",
    "            # - If done, Q-target = reward (no future reward)\n",
    "            # - Otherwise, Q-target = reward + gamma * Q_target(S', argmax Q_eval(S', a))\n",
    "\n",
    "            # Predict current Q-values for state using eval_model\n",
    "            # Use eval_model to determine best action in next_state\n",
    "            # Use target_model to compute Q-value for that action\n",
    "\n",
    "            # Update only the Q-value for the taken action\n",
    "\n",
    "            # Fit the model:\n",
    "            # - Inputs: state\n",
    "            # - Targets: updated Q-values (with action Q-value replaced by computed target)\n",
    "\n",
    "            # Update exploration rate\n",
    "            # if epsilon > epsilon_min:\n",
    "            #     epsilon *= epsilon_decay\n",
    "\n",
    "            # Periodically update the target network\n",
    "            # if train_counter % target_update_freq == 0:\n",
    "            #     target_model.set_weights(eval_model.get_weights())\n",
    "\n",
    "    # record end time and log training time\n",
    "    end = time.time()\n",
    "    total_training_time += end - start\n",
    "\n",
    "    # Evaluation\n",
    "    # [WriteCode]   \n",
    "    xqy_policy.evaluate()\n",
    "    \n",
    "#     print(f\"Episode {ep + 1}/{episode} | Ep. Total Reward: {total_reward}\"\n",
    "#         f\" | Epsilon : {epsilon:.3f}\")\n",
    "# #        f\" | Eval Rwd Mean: {eval_reward_mean:.2f}\"\n",
    "# #        f\" | Eval Rwd Var: {eval_reward_var:.2f}\")\n",
    "\n",
    "    # Log\n",
    "#    eval_reward_mean_lst.append(eval_reward_mean)\n",
    "#    eval_reward_var_lst.append(eval_reward_var)\n",
    "    train_reward_lst.append(total_reward)\n",
    "\n",
    "    # Early Stopping Condition to avoid overfitting\n",
    "    # If the evaluation reward reaches the specified threshold, stop training early.\n",
    "    # The default threshold is set to 500, but you should adjust this based on observed training performance.\n",
    "#    if eval_reward_mean > 500: # [Modify this threshold as needed]\n",
    "#        print(f\"Early stopping triggered at Episode {ep + 1}.\")\n",
    "#        break\n",
    "\n",
    "# record end time and calculate average training time per episode\n",
    "# evaluate average training time per episode\n",
    "print(f\"Training time: {total_training_time/episode:.4f} seconds per episode\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oKfbQWJq363X"
   },
   "source": [
    "## Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L7qIOwiX363Y"
   },
   "outputs": [],
   "source": [
    "# Write code to plot\n",
    "# 1) Moving Averaged Training Reward, 2) Evaluation Mean, 3) Evaluation Variance\n",
    "# [Write Code]\n",
    "\n",
    "# plot_smoothed_training_rwd(...\n",
    "\n",
    "# plot_eval_rwd_mean(...\n",
    "\n",
    "# plot_eval_rwd_var(...\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from visualization import GraphPloter\n",
    "\n",
    "gp = GraphPloter()\n",
    "def plot_eval_rwd_mean(eval_mean_list):\n",
    "    \"\"\"Plot evaluation reward mean.\"\"\"\n",
    "    gp.plot_eval_rwd_mean(eval_mean_list)\n",
    "\n",
    "\n",
    "def plot_eval_rwd_var(eval_var_list):\n",
    "    \"\"\"Plot evaluation reward variance.\"\"\"\n",
    "    gp.plot_eval_rwd_var(eval_var_list)\n",
    "\n",
    "\n",
    "def plot_smoothed_training_rwd(train_rwd_list, window_size=20):\n",
    "    \"\"\"Plot smoothed training rewards using a moving average.\"\"\"\n",
    "    gp.plot_smoothed_training_rwd(train_rwd_list, window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YgVEqsvQHsiD"
   },
   "source": [
    "# DDQN - Alternative\n",
    "\n",
    "You may insert extra cells in the notebook to perform tuning experiments and log results effectively. Use TensorBoard, plots, or tables to visualize the impact of different hyperparameter choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0xno9cGCHrx5"
   },
   "outputs": [],
   "source": [
    "# [Write Code]\n",
    "from visualization import Comparator\n",
    "# xqy_comparator = Comparator(base_policy, xqy_policy)\n",
    "# xqy_comparator.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5MBGp2LfHxDr"
   },
   "source": [
    "## Comparison\n",
    "\n",
    "Compare performance of the Baseline and the Alternative DDQN Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3NDTwhmwH3Lk"
   },
   "outputs": [],
   "source": [
    "# [Write Code]\n",
    "from visualization import Comparator\n",
    "xqy_comparator = Comparator(xqy_policy, xqy_policy_planb)\n",
    "xqy_comparator.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUo4vcwQnbWV"
   },
   "source": [
    "# Visualize with Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZcIJ3DQc3-Jw"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./eec4400_logs --port=6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-insiT7D_lq"
   },
   "source": [
    "# Comparison Across Four Alternative Policies\n",
    "\n",
    "Compare hyperparameters and performance of the four alternative policies.\n",
    "\n",
    "You may insert extra cells in the notebook to tabulate/plot/log results effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SjQwsaxSE5Jh"
   },
   "outputs": [],
   "source": [
    "# [Write Code]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "tfdlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
