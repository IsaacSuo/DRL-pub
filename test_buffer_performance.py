#!/usr/bin/env python3
"""
ç»éªŒå›æ”¾ç¼“å†²åŒºæ€§èƒ½æµ‹è¯•
æ¯”è¾ƒä¼˜åŒ–å‰åçš„é‡‡æ ·æ€§èƒ½
"""

import time
import numpy as np
import tensorflow as tf
from collections import deque

def old_buffer_sample(buffer, batch_size):
    """åŸå§‹çš„é‡‡æ ·æ–¹æ³•ï¼ˆä»kytolly.pyå¤åˆ¶ï¼‰"""
    indices = np.random.choice(len(buffer), batch_size, replace=False)
    minibatch = [buffer[i] for i in indices]

    states_batch = np.stack([experience[0].squeeze() for experience in minibatch], axis=0)
    actions_batch = np.array([experience[1] for experience in minibatch], dtype=np.int32)
    rewards_batch = np.array([experience[2] for experience in minibatch], dtype=np.float32)
    next_states_batch = np.stack([experience[3].squeeze() for experience in minibatch], axis=0)
    dones_batch = np.array([experience[4] for experience in minibatch], dtype=np.float32)

    return (
        tf.convert_to_tensor(states_batch, dtype=tf.float32),
        tf.convert_to_tensor(actions_batch, dtype=tf.int32),
        tf.convert_to_tensor(rewards_batch, dtype=tf.float32),
        tf.convert_to_tensor(next_states_batch, dtype=tf.float32),
        tf.convert_to_tensor(dones_batch, dtype=tf.float32)
    )

def benchmark_performance():
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    print("ğŸš€ ç»éªŒå›æ”¾ç¼“å†²åŒºæ€§èƒ½æµ‹è¯•")
    print("=" * 50)

    # æµ‹è¯•å‚æ•°
    buffer_size = 10000
    batch_size = 64
    num_samples = 100
    state_dim = 4

    print(f"ğŸ“Š æµ‹è¯•å‚æ•°:")
    print(f"   ç¼“å†²åŒºå¤§å°: {buffer_size}")
    print(f"   æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"   æµ‹è¯•æ¬¡æ•°: {num_samples}")
    print(f"   çŠ¶æ€ç»´åº¦: {state_dim}")
    print()

    # 1. ç”Ÿæˆæµ‹è¯•æ•°æ®
    print("ğŸ“¦ ç”Ÿæˆæµ‹è¯•æ•°æ®...")
    test_data = []
    for i in range(buffer_size):
        state = np.random.randn(state_dim)
        action = np.random.randint(0, 2)
        reward = np.random.randn()
        next_state = np.random.randn(state_dim)
        done = np.random.randint(0, 2)
        test_data.append((state, action, reward, next_state, done))

    # 2. æµ‹è¯•åŸå§‹dequeæ–¹æ³•
    print("ğŸŒ æµ‹è¯•åŸå§‹dequeæ–¹æ³•...")
    old_buffer = deque(test_data)

    start_time = time.time()
    for _ in range(num_samples):
        old_buffer_sample(old_buffer, batch_size)
    old_time = time.time() - start_time

    # 3. æµ‹è¯•ä¼˜åŒ–åçš„æ–¹æ³•
    print("âš¡ æµ‹è¯•ä¼˜åŒ–åçš„ç¼“å†²åŒº...")
    from agent.replay_buffer import OptimizedReplayBuffer
    new_buffer = OptimizedReplayBuffer(buffer_size, state_dim)

    # æ·»åŠ æ•°æ®åˆ°æ–°ç¼“å†²åŒº
    for state, action, reward, next_state, done in test_data:
        new_buffer.add(state, action, reward, next_state, done)

    start_time = time.time()
    for _ in range(num_samples):
        new_buffer.sample(batch_size)
    new_time = time.time() - start_time

    # 4. ç»“æœå¯¹æ¯”
    print("\nğŸ“ˆ æ€§èƒ½å¯¹æ¯”ç»“æœ:")
    print("=" * 50)
    print(f"åŸå§‹æ–¹æ³•è€—æ—¶: {old_time:.4f} ç§’")
    print(f"ä¼˜åŒ–æ–¹æ³•è€—æ—¶: {new_time:.4f} ç§’")
    print(f"æ€§èƒ½æå‡:     {old_time/new_time:.2f}x")
    print(f"æ—¶é—´èŠ‚çœ:     {((old_time - new_time) / old_time * 100):.1f}%")

    # å•æ¬¡é‡‡æ ·å¹³å‡æ—¶é—´
    old_avg = old_time / num_samples * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
    new_avg = new_time / num_samples * 1000
    print(f"\nå•æ¬¡é‡‡æ ·å¹³å‡æ—¶é—´:")
    print(f"åŸå§‹æ–¹æ³•: {old_avg:.3f} ms")
    print(f"ä¼˜åŒ–æ–¹æ³•: {new_avg:.3f} ms")

    # å†…å­˜ä½¿ç”¨ä¼°ç®—
    print(f"\nğŸ’¾ å†…å­˜ä½¿ç”¨ä¼°ç®—:")
    print(f"åŸå§‹æ–¹æ³•: ~{buffer_size * 5 * 8 / 1024 / 1024:.2f} MB (Pythonå¯¹è±¡ + å¼€é”€)")
    print(f"ä¼˜åŒ–æ–¹æ³•: ~{buffer_size * (4 + 4 + 4 + 4 + 4) / 1024 / 1024:.2f} MB (é¢„åˆ†é…æ•°ç»„)")

    return old_time / new_time

if __name__ == "__main__":
    speedup = benchmark_performance()
    print(f"\nâœ… ä¼˜åŒ–æˆåŠŸï¼æ€§èƒ½æå‡ {speedup:.2f}x")