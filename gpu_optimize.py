"""
GPUÊÄßËÉΩ‰ºòÂåñÈÖçÁΩÆËÑöÊú¨
Âú®ËÆ≠ÁªÉËÑöÊú¨ÂºÄÂ§¥ÂØºÂÖ•Ê≠§Ê®°Âùó‰ª•ÂêØÁî®ÊâÄÊúâGPU‰ºòÂåñ
Usage:
    from gpu_optimize import configure_gpu_optimization
    configure_gpu_optimization()
"""

import os
import tensorflow as tf


def configure_gpu_optimization(
    enable_xla: bool = True,
    enable_mixed_precision: bool = False,
    memory_growth: bool = True,
    verbose: bool = True
):
    """
    ÈÖçÁΩÆTensorFlowÁöÑGPU‰ºòÂåñÈÄâÈ°π

    Args:
        enable_xla: ÂêØÁî®XLA(Âä†ÈÄüÁ∫øÊÄß‰ª£Êï∞)ÁºñËØë‰ºòÂåñ (Êé®Ëçê: True)
        enable_mixed_precision: ÂêØÁî®Ê∑∑ÂêàÁ≤æÂ∫¶ËÆ≠ÁªÉ (ÈúÄË¶ÅTensor CoreÊîØÊåÅ)
        memory_growth: ÂêØÁî®GPUÂÜÖÂ≠òÊåâÈúÄÂ¢ûÈïø,Èò≤Ê≠¢OOM (Êé®Ëçê: True)
        verbose: ÊâìÂç∞‰ºòÂåñÈÖçÁΩÆ‰ø°ÊÅØ

    Returns:
        dict: ‰ºòÂåñÈÖçÁΩÆÁä∂ÊÄÅ
    """

    optimization_status = {}

    # 1. XLAÁºñËØë‰ºòÂåñ
    if enable_xla:
        os.environ['TF_XLA_FLAGS'] = '--tf_xla_auto_jit=2'
        tf.config.optimizer.set_jit(True)
        optimization_status['XLA'] = '‚úÖ Enabled'
        if verbose:
            print("‚úÖ XLA compilation enabled (expected 5-15% speedup)")
    else:
        optimization_status['XLA'] = '‚ùå Disabled'

    # 2. GPUÂÜÖÂ≠òÂ¢ûÈïø
    gpus = tf.config.list_physical_devices('GPU')
    if gpus:
        optimization_status['GPUs_found'] = len(gpus)

        if memory_growth:
            try:
                for gpu in gpus:
                    tf.config.experimental.set_memory_growth(gpu, True)
                optimization_status['Memory_growth'] = '‚úÖ Enabled'
                if verbose:
                    print(f"‚úÖ Configured {len(gpus)} GPU(s) with memory growth enabled")
            except RuntimeError as e:
                optimization_status['Memory_growth'] = f'‚ö†Ô∏è  Warning: {e}'
                if verbose:
                    print(f"‚ö†Ô∏è  GPU memory growth warning: {e}")
        else:
            optimization_status['Memory_growth'] = '‚ùå Disabled'
    else:
        optimization_status['GPUs_found'] = 0
        optimization_status['Device'] = 'CPU'
        if verbose:
            print("‚ö†Ô∏è  No GPU detected, running on CPU")

    # 3. Ê∑∑ÂêàÁ≤æÂ∫¶ËÆ≠ÁªÉ
    if enable_mixed_precision:
        try:
            from tensorflow.keras import mixed_precision
            policy = mixed_precision.Policy('mixed_float16')
            mixed_precision.set_global_policy(policy)
            optimization_status['Mixed_precision'] = '‚úÖ Enabled (float16)'
            if verbose:
                print("‚úÖ Mixed precision training enabled (float16)")
                print("   Note: Requires Tensor Cores for optimal performance")
        except Exception as e:
            optimization_status['Mixed_precision'] = f'‚ùå Failed: {e}'
            if verbose:
                print(f"‚ö†Ô∏è  Mixed precision failed: {e}")
    else:
        optimization_status['Mixed_precision'] = '‚ùå Disabled'

    # 4. ÂÖ∂‰ªñÊÄßËÉΩ‰ºòÂåñ
    # ÂêØÁî®TensorFlowÁöÑÁ°ÆÂÆöÊÄßÊìç‰Ωú(ÂèØÈÄâ,ÂèØËÉΩÁï•ÂæÆÈôç‰ΩéÊÄßËÉΩ)
    # os.environ['TF_DETERMINISTIC_OPS'] = '1'

    # Á¶ÅÁî®‰∏çÂøÖË¶ÅÁöÑÊó•Âøó
    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

    # Á∫øÁ®ã‰ºòÂåñ
    tf.config.threading.set_inter_op_parallelism_threads(0)  # Ëá™Âä®
    tf.config.threading.set_intra_op_parallelism_threads(0)  # Ëá™Âä®

    optimization_status['Threading'] = 'Auto-configured'

    if verbose:
        print("\n" + "="*60)
        print("GPU Optimization Summary:")
        print("="*60)
        for key, value in optimization_status.items():
            print(f"  {key:20s}: {value}")
        print("="*60 + "\n")

    return optimization_status


def benchmark_gpu_performance(iterations: int = 100, batch_size: int = 32):
    """
    ÁÆÄÂçïÁöÑGPUÊÄßËÉΩÂü∫ÂáÜÊµãËØï

    Args:
        iterations: ÊµãËØïËø≠‰ª£Ê¨°Êï∞
        batch_size: ÊâπÊ¨°Â§ßÂ∞è

    Returns:
        float: Âπ≥ÂùáÊØèÊ¨°Ëø≠‰ª£Êó∂Èó¥(ÊØ´Áßí)
    """
    import time
    import numpy as np

    print(f"\nüîç Running GPU benchmark ({iterations} iterations, batch_size={batch_size})...")

    # ÂàõÂª∫ÊµãËØïÊï∞ÊçÆ
    x = tf.random.normal([batch_size, 4])
    y = tf.random.normal([batch_size, 2])

    # ÁÆÄÂçïÁöÑÊµãËØïÊ®°Âûã
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dense(2)
    ])

    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)
    loss_fn = tf.keras.losses.MeanSquaredError()

    @tf.function(jit_compile=True)
    def train_step(x, y):
        with tf.GradientTape() as tape:
            predictions = model(x, training=True)
            loss = loss_fn(y, predictions)
        gradients = tape.gradient(loss, model.trainable_variables)
        optimizer.apply_gradients(zip(gradients, model.trainable_variables))
        return loss

    # È¢ÑÁÉ≠
    for _ in range(10):
        train_step(x, y)

    # Âü∫ÂáÜÊµãËØï
    start_time = time.time()
    for _ in range(iterations):
        train_step(x, y)
    end_time = time.time()

    avg_time_ms = (end_time - start_time) / iterations * 1000

    print(f"‚úÖ Benchmark complete: {avg_time_ms:.3f} ms/iteration")
    print(f"   Throughput: {1000/avg_time_ms:.1f} iterations/second\n")

    return avg_time_ms


if __name__ == "__main__":
    # ËøêË°å‰ºòÂåñÈÖçÁΩÆ
    print("="*60)
    print("DRL GPU Optimization Configuration")
    print("="*60 + "\n")

    status = configure_gpu_optimization(
        enable_xla=True,
        enable_mixed_precision=False,  # ÈªòËÆ§ÂÖ≥Èó≠,ÈúÄË¶ÅÊâãÂä®ÂêØÁî®
        memory_growth=True,
        verbose=True
    )

    # ËøêË°åÂü∫ÂáÜÊµãËØï
    if status.get('GPUs_found', 0) > 0:
        benchmark_gpu_performance(iterations=100, batch_size=32)
    else:
        print("‚ö†Ô∏è  Skipping benchmark (no GPU available)")
